\section{Literature Review}

   \subsection{Introduction to Time Series Analysis}

      \cite{Rasmussen2006} We have a training set of \(\mathcal{D}\) of \(n\) observations, \(\mathcal{D} = \{(\mathbf{x}_{i}, y_{i}) \, | \, i = 1, \, \ldots, \, n\}\), where \(\mathbf{x}\) denotes the input vector (covariates) of dimension \(\mathcal{D}\) and y denotes a scalar output or target (dependent variable); the column vector of inputs for all n cases are arranged in the \(\mathcal{D} \times n \, design \, matrix\, X\), and the targets are collected in the vector \(\mathbf{y}\), so we can write \(\mathcal{D} = (X, \mathbf{y})\). 

      \vspace{1em}

      \textbf{Definition 2.1.1} \cite{Watson2025} \textit{A time series is a sequence of observations collected at regular equally spaced intervals over a period of time.}

      \vspace{1em}

      Time series data are extremely common. They arize in virtually every application field, such as business (sales figures, production numbers, and customer frequencies), economics (stock prices, exchange rates, and interest rates), and official statistics (census data, personal expenditures, and road casualities).

      \vspace{1em}

      \textbf{Definition 2.1.2} \cite{Watson2025} \textit{Time series analysis is a collection of statistical techniques that attempt to isolate and quantify the influence of events and changes in conditions in order to build a model that utilizes this information to forecast future values of the time series.}

      \vspace{1em}

      Standard inferential techniques which assume independence of observations (e.g. regression) do not work well when data is collected at regular equally spaced time intervals because the observations are likely to be dependent. When this dependence occurs between observations \(g\) time periods apart, it is called `autocorrelation' at lag \(g\). So we cannot assume that the data consitute a random sample.

      \vspace{1em}

      The basic assumption underlying time series forecasting is that the factors that influence patterns of activity in past and present will continue to do so in more or less the same manner in the future. Thus the overall purpose of time series analysis is to identify and isolate these influencing factors from the past in order to better understand the process underlying the time series, for predictive purposes.

      \vspace{1em}

      \textbf{Definition 2.1.3} \cite{Watson2025} \textit{A time series is said to be stationary if it's statistical properties are constant over time. This implies that the time series has a constant mean and variance over time.}

      \vspace{1em}

      Most time series we encounter are non-stationary and we often need to transform them so that they exhibit stationarity. These transformations enable us to consider what other information exists in the data after we have removed the effect of a trend or seasonality and/or changing variance.

      \subsubsection*{2.1.1 Time series plot}

      The first step in time series analysis is to plot the data and observe any patterns that have occurred over time using a line graph. The time series plot enables us to detect and describe components of past behavior of the series. The identified components help in finding a suitable statistical model to decribe the data, which enables us to forecast future values of the time series.

      \vspace{1em}

      Components of a non-stationary time series are:
      \begin{itemize}
         \item Trend
         \item Cyclical variation
         \item Seasonal variation
         \item Random variation
      \end{itemize}

      \vspace{1em}

      \textbf{\textit{Trend}}: The long-term tendency of a time series. The pattern observed may move steadily in an upward or downward direction, or stay the same over time.

      \vspace{1em}

      \textbf{\textit{Cyclical variation}}: Irregular long-term wavelike movements through a time series. Due to extendend periods of prosperity followed by extended periods of recession.

      \vspace{1em}

      \textbf{\textit{Seasonal variation}}: Regular short-term repetitive wavelike movements through a time series, often when data is recorded houly, daily, weekly, monthly or quarterly. It repeats itself through the time series.

      \vspace{1em}

      \textbf{\textit{Random variation}}: Random variations in the data due to the combined effects of all unforeseen events such as war, strikes, natural disasters, power cuts, etc. 

      \vspace{1em}

      The simplest assumption about the relationship between the components in a time series is that they are additive and independent of each other. We write the additive model as: \[Y_{t} = T_{t} + C_{t} + S_{t} + R_{t},\] where \(t\) is the time period we are interested in, \(Y_{t}\) is the observed value of the time series at time \(t\), \(T_{t}\) is the value of the trend component at time \(t\), \(C_{t}\) is the value of the cyclical component at time \(t\), \(S_{t}\) is the value of the seasonal component at time \(t\), and \(R_{t}\) is the value of the random component at time \(t\).

      \vspace{1em}

      Alternatively, we can assume that the four components of the time series are not necessarily independent and can affect one another. This is captured by the multiplicative model as: \[Y_{t} = T_{t} \times C_{t} \times S_{t} \times R_{t}.\] Note that this can be made additive by taking the logarithm of the series.

      \vspace{1em}

      The additive model is most appropriate if the magnitude of the seasonal fluctuations or variation around the trend-cycle does not vary with the level of the time series. When the variation in seasonal pattern, or the variation around the trend-cycle, appears to be proportional to the level of the time series, then a multiplicative model is more appropriate. 

   \subsection{Simple forecasting models}

      \cite{Watson2025} There are some forecasting methods that are simple, yet remarkably effective for some time series.

      \vspace{1em}
      
      We fit simple forecasting methods to use as `benchmarks' against which we compare other more complex forecasting methods i.e. if a more complicated forecasting method does not yield better forecasts than one of the simple methods here, there is no need to use it for the particular time series analysed.

      \vspace{1em}

      There are four simple forecasting methods that we consider:
      \begin{itemize}
         \item The average method
         \item The naive method
         \item The seasonal naive method
         \item The drift method
      \end{itemize}

      \vspace{1em}

      \textbf{\textit{Average method}}: the forecasts of all future values is the mean of the historical data: \[\hat{y}_{T+h|T} = \frac{\sum_{t=1}^{T} y_{t}}{T}.\]

      The notation \(\hat{y}_{T+h|T}\) is a short-hand for the estimate of \(y_{T+h|T}\) based on the data \(\{y_{1},\, \ldots,\, y_{T}\}\).

      \vspace{1em}

      \textbf{\textit{Naive method}}: the forecasts of all future values is the last observation: \[\hat{y}_{T+h|T} = y_{T}.\]

      \textbf{\textit{Seasonal naive method}}: this is similar to the naive method, used when we have a time series with a strong seasonal component. The forecasted value for a particular `season' is simply the value corresponding to the previous `season': \[\hat{y}_{T+h|T} = y_{T+h-m(k+1)},\]

      Where \(m\) is the seasonal period and \(k\) is the integer part of \((h-1)/m\) i.e. the number of complete seasonal variations that have passed since the end of the original time series data.

      \vspace{1em}

      For example, if our time series is montly data and a seasonal variation lasts 12 months, then the future forecasts for all April values is simply the previous April value. If our time series is quartely data and a season lasts 4 quarters, then the future forecasts for all quarter 3 values is the previous quarter 3 value etc.

      \vspace{1em}

      \textbf{\textit{Drift method}}: An extension of the naive method to allow for the presence of a linear trend in the data. The amount of change over time (called the `drift') is equal to the average change in the historical data: \[\hat{y}_{T+h|T} = y_{T} + h \Big(\frac{y_{T} - y_{1}}{T-1} \Big).\]

   \subsection{Gaussian Processes}

      \textbf{Definition 2.3.1} \cite{Rasmussen2006} \textit{A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.}

      \vspace{1em}

      A Gaussian process is completely specified by its mean function and co-variance function. We define mean mean function \(m(x_{i})\) and the covariance function \(k(x_{i},\, x_{j})\) for a real process \(f(x_{i})\) as
      \[m(x_{i}) = \mathbb{E}[f(x_{i})],\]
      \[k(x_{i},\, x_{j}) = \mathbb{E}[(f(x_{i}) - m(x_{i}))(f(x_{j}) - m(x_{j}))],\]
      and will write the Gaussian process as
      \[f(x_{i}) \sim \text{GP}(m(x_{i}), \, k(x_{i},\, x_{j})).\]

      \(\mathbf{x} =
      \begin{bmatrix}
         X_{1} \\
         \vdots \\
         X_{n}
      \end{bmatrix}
      \:
      \sim \mathcal{N}_{n}(\mathbf{m}, \, \mathbf{K}),
      \:
      \text{where}
      \:
      \mathbf{m} = 
      \begin{bmatrix}
         m(x_{1}) \\
         m(x_{2}) \\
         \vdots \\
         m(x_{n})
      \end{bmatrix}  
      \: 
      \text{and}
      \:
      \mathbf{K} = 
      \begin{bmatrix}
         k(x_{1}, \, x_{1}) & k(x_{1}, \, x_{2}) & \cdots & k(x_{1}, \, x_{n}) \\
         k(x_{2}, \, x_{1}) & k(x_{2}, \, x_{2}) & \cdots & k(x_{2}, \, x_{n}) \\
         \vdots & \vdots & \ddots & \vdots \\
         k(x_{n}, \, x_{1}) & k(x_{n}, \, x_{2}) & \cdots & k(x_{n}, \, x_{n})
      \end{bmatrix}\).

      \subsubsection{Covariance functions}

         \cite{Roberts2013} In the following section, we briefly describe commonly used kernels. We start with a simple white noise, and then consider common \textit{stationary} covariances, both uni- and multi-dimensional. We finish this section with periodic and quasi-periodic kernel functions. We note that sums (and products) of valid covariance kernels give valid covariance functions (i.e. the resultant covariance matrices are positive semi-definite) and so we may entertain with ease multiple explanatory hypothesis. 

         \vspace{1em}

         \textit{White noise} with variance \(\sigma^2\) is represented by
         
         \[k(x_{i}, \, x_{j}) = \sigma^2\delta_{ij}, \, \text{where} \, \delta_{ij} = 1 \, \text{for} \, i = j \, \text{and} \, \delta_{ij} = 0 \, \text{for} \, i \neq j.\]

         This kernel allows us to entertain uncertainity in our observed data and is so typically found added to other kernels.

         \vspace{1em}

         \textit{The SE kernel} is given by

         \[k(x_{i}, \, x_{j}) = h^{2} \text{exp} \bigg(- \Big(\frac{x_i - x_j}{\lambda} \Big)^2 \bigg),\]

         where \(h\) is an output-scale amplitude and \(\lambda\) is an input (length, or time) scale. This gives rather smooth variations with typical time scale of \(\lambda\) and admits functions drawn from the GP that are infinitely differentiable.

         \vspace{1em}

         \textit{The rational quadratic (RQ) kernel} is given by

         \[k(x_{i}, \, x_{j}) = h^{2} \bigg(1 + \frac{(x_{i} - x_{j})^2}{\alpha\lambda^2} \bigg)^{-\alpha},\]

         where \(\alpha\) is known as the index. \cite{Rasmussen2006} show that this is equivalent to a scale mixture of SE kernels with different length scales, the latter distributed according to a Beta distribution with parameters \(\alpha\) and \(\lambda^{-2}\). This gives variations with a range of time scales, the distribution peaking around \(\lambda\) but extending significantly longer period (but remaining rather smooth). When \(\alpha \to \infty\), the RQ kernel reduces to the SE kernel with length scale \(\lambda\).

         \vspace{1em}

         \textit{The Mat\'ern} class of covariance functions is defined by

         \[k(x_{i}, \, x_{j}) = h^{2} \frac{1}{\Gamma(v)2^{v-1}} \bigg(2\sqrt{v}\frac{|x_{i} - x_{j}|}{\lambda} \bigg) \mathbb{B}_{v} \bigg(2\sqrt{v}\frac{|x_{i} - x_{j}|}{\lambda} \bigg),\]

         where \(h\) is the output scale, \(\lambda\) is the input scale, \(\Gamma()\) is the standard Gamma function and \(\mathbb{B}()\) is the modified Bessel function of second order. The additional hyperparameter \(v\) controls the degree of differentiability of the resultant functions modelled by a GP with a Mat\'ern covariance function, such that there are only \((v + \frac{1}{2})\) times differentiable. As \(v \to \infty\), so the functions become infinitely differentiable and the Matern kernel becomes the SE one. Taking \(v = \frac{1}{2}\) gives the exponential kernel

         \[k(x_{i}, \, x_{j}) = h^{2} \text{exp} \bigg(- \Big(\frac{x_i - x_j}{\lambda} \Big)^2 \bigg),\]

         which results in functions that are only once differentiable, and corresponds to the Ornstein-Unlenbeck process.

      \subsubsection{Prediction}

         \cite{Betancourt2020} To simulate sampling a function from a Gaussian process and then evaluating the sampled function at the grid of covariate values we can just directly sample from a multivariate normal random number generator.

         \vspace{1em}

         We can also consider \textit{predictions} by taking advantage of the conditional structure of a multivariate normal density function. Consider a grid of observed covariate values \[\{x_{1}^{\text{obs}}, \, \ldots, \, x_{n_{\text{obs}}}^{\text{obs}}\}\]

         where we know the function values \(f(x_{i}^{\text{obs}})\), and a grid of unobserved covariate values where we want to predict the functional values, \[\{x_{1}^{\text{pred}}, \, \ldots, \, x_{n_{\text{pred}}}^{\text{pred}}\}.\]

         The parameters of the multivariate normal density function of the combined covariate values decomposes into the parameters of the multivariate normal density function for each component grid plus mixed covariate function evaluations.

         \vspace{1em}

         \(\mathbf{m} = 
         \begin{bmatrix}
            m(x_{1}^{\text{obs}}) \\
            \vdots \\
            m(x_{n_{\text{obs}}}^{\text{obs}}) \\
            m(x_{1}^{\text{pred}}) \\
            \vdots \\
            m(x_{n_{\text{pred}}}^{\text{pred}})
         \end{bmatrix}  
         \: 
         \text{and}
         \:
         \mathbf{K} = 
         \begin{bmatrix}
            k(x_{1}^{\text{obs}}, \, x_{1}^{\text{obs}}) 
            & \cdots 
            & k(x_{1}^{\text{obs}}, \, x_{n_{\text{obs}}}^{\text{obs}}) 
            & k(x_{1}^{\text{obs}}, \, x_{1}^{\text{pred}}) 
            & \cdots 
            & k(x_{1}^{\text{obs}}, \, x_{n_{\text{pred}}}^{\text{pred}}) \\
            
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
            
            k(x_{n_{\text{obs}}}^{\text{obs}}, \, x_{1}^{\text{obs}}) 
            & \cdots 
            & k(x_{n_{\text{obs}}}^{\text{obs}}, \, x_{n_{\text{obs}}}^{\text{obs}}) 
            & k(x_{n_{\text{obs}}}^{\text{obs}}, \, x_{1}^{\text{pred}}) 
            & \cdots 
            & k(x_{n_{\text{obs}}}^{\text{obs}}, \, x_{n_{\text{pred}}}^{\text{pred}}) \\
            
            k(x_{1}^{\text{pred}}, \, x_{1}^{\text{obs}}) 
            & \cdots 
            & k(x_{1}^{\text{pred}}, \, x_{n_{\text{obs}}}^{\text{obs}}) 
            & k(x_{1}^{\text{pred}}, \, x_{1}^{\text{pred}}) 
            & \cdots 
            & k(x_{1}^{\text{pred}}, \, x_{n_{\text{pred}}}^{\text{pred}}) \\
            
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
            
            k(x_{n_{\text{pred}}}^{\text{pred}}, \, x_{1}^{\text{obs}}) 
            & \cdots 
            & k(x_{n_{\text{pred}}}^{\text{pred}}, \, x_{n_{\text{obs}}}^{\text{obs}}) 
            & k(x_{n_{\text{pred}}}^{\text{pred}}, \, x_{1}^{\text{pred}}) 
            & \cdots 
            & k(x_{n_{\text{pred}}}^{\text{pred}}, \, x_{n_{\text{pred}}}^{\text{pred}})
         \end{bmatrix}\).

         \vspace{1em}
         
         Mmore compactly

         \vspace{1em}

         \(\mathbf{m} = 
         \begin{bmatrix}
            \mathbf{m}_{\text{obs}} \\
            \\
            \mathbf{m}_{\text{pred}}
         \end{bmatrix}  
         \: 
         \text{and}
         \:
         \mathbf{K} = 
         \begin{bmatrix}
            \mathbf{K}_{\text{obs}} & \mathbf{K}_{\text{mix}} \\
            \\
            (\mathbf{K}_{\text{mix}})^{\text{T}} & \mathbf{K}_{\text{pred}}
         \end{bmatrix}\).

         \vspace{1em}

         The conditional probability density function for the unobserved function values \((f_{\text{pred}})_{i} = f(x_{i}^{\text{pred}})\) given the observed function values  \((f_{\text{obs}})_{i} = f(x_{i}^{\text{obs}})\) falls into the multivariate normal family,

         \[\pi(\mathbf{f}_{\text{pred}} | \mathbf{f}_{\text{obs}}) \sim \mathcal{N}_{n}(\boldsymbol{\mu}, \boldsymbol{\Sigma}),\]

         with the location parameters 

         \[\boldsymbol{\mu} = \mathbf{m}_{\text{pred}} + (\mathbf{K}_{\text{mix}})^{\text{T}} \cdot (\mathbf{K}_{\text{obs}})^{-1} \cdot (\mathbf{f}_{\text{obs}} - \mathbf{m}_{\text{obs}}),\]

         and the covariance matrix parameters

         \[\boldsymbol{\Sigma} = \mathbf{K}_{\text{pred}} - (\mathbf{K}_{\text{mix}})^{\text{T}} \cdot (\mathbf{K}_{\text{obs}})^{-1} \cdot \mathbf{K}_{\text{mix}}.\]

         Inference with Gaussian process priors proceeds similarly. Once we've identified the covariate values at which we have observations or will want to make predictions we can specify the marginalized prior model with the corresponding multivariate normal density function. If the observational model is normal then we can generate predictions analytically with the conditioning operations for the posterior covariance function.
