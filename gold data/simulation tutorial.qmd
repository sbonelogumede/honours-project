---
title: "Gaussian process regression in brms"
format: html
editor: visual
 math: mathjax
---

```{r, include=FALSE}
# install packages from CRAN (unless installed)
pckgs_needed <- c(
  "tidyverse",
  "brms",
  "rstan",
  "rstanarm",
  "remotes",
  "tidybayes",
  "bridgesampling",
  "shinystan",
  "mgcv"
)
pckgs_installed <- installed.packages()[,"Package"]
pckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]
if(length(pckgs_2_install)) {
  install.packages(pckgs_2_install)
} 

# install additional packages from GitHub (unless installed)
if (! "aida" %in% pckgs_installed) {
  remotes::install_github("michael-franke/aida-package")
}
if (! "faintr" %in% pckgs_installed) {
  remotes::install_github("michael-franke/faintr")
}
if (! "cspplot" %in% pckgs_installed) {
  remotes::install_github("CogSciPrag/cspplot")
}



# load the required packages
x <- lapply(pckgs_needed, library, character.only = TRUE)
library(aida)
library(faintr)
library(cspplot)

# these options help Stan run faster
options(mc.cores = parallel::detectCores())

# use the CSP-theme for plotting
theme_set(theme_csp())

# global color scheme from CSP
project_colors = cspplot::list_colors() |> pull(hex)
# names(project_colors) <- cspplot::list_colors() |> pull(name)

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
   scale_fill_manual(..., values = project_colors)
}
```

Gaussian process (GP) allows us to specify a vast amount of non-linear curves, so to speak. More concretely, a GP, defined by a kernel \\( k(\\cdot) \\), a mean function \\( m(\\cdot) \\), and the parameters \\( \\sigma_f \\) and \\( \\lambda \\), implies a prior of functions. This is very abstract and best explored through simulation.

\\\[
k(\\cdot),
\\\]

\\\[
m(\\cdot),
\\\]

\\\[
\\sigma_f \\quad \\text{and} \\quad \\lambda,
\\\]

implies a prior of functions. This is very abstract and best explored through simulation.

 `get_GP_simulation` and it samples from a Gaussian process regression.It takes as input a vector `x` to generated predictions for, values for the kernel parameters `sigma_f` and `lambda` and also the usual simple linear regression parameters `Intercept`, `slope` and `sigma`. 

```{r}
get_GP_simulation <- 
  function(x = seq(0,10, by = 0.1), 
           Intercept = 0, 
           slope = 1, 
           sigma = 1, 
           sigma_f=0.5, 
           lambda=100, 
           seed = NULL) {
    
    if (! is.null(seed)){
      set.seed(seed)
    }
    
    # number of points to generate prediction for
    N <- length(x)
    
    # linear predictor (vanilla LM)
    eta = Intercept + slope * x
    
    # kernel function (here: radial basis function kernel)
    get_covmatrix <- function(x, sigma_f, lambda) {
      K = matrix(0, nrow=N, ncol=N)
      for (i in 1:N) {
        for (j in 1:N) {
          K[i,j] = sigma_f^2 * exp(sqrt((i-j)^2) / (-2 *lambda))
        }
      }
      return(K)
    }
    
    # covariance matrix
    K <- get_covmatrix(x, sigma_f, lambda)
    
    # Gaussian process wiggles
    epsilon_gp <- mvtnorm::rmvnorm(
      n     = 1, 
      mean  = rep(0, N),
      sigma = K)[1,]
    
    # central tendency
    mu <- epsilon_gp + eta
    
    # data prediction
    y <- rnorm(N, mu, sigma)
      
    tibble(x, eta, mu, y) 
  }

plot_GP_simulation <- function(GP_simulation) {
  GP_simulation |> 
    ggplot(aes(x = x, y = eta )) + 
    geom_line(color = project_colors[1], size = 1.25) +
    geom_line(aes(y = mu), color = project_colors[2], size = 1.25) +
    geom_point(aes(y = y), color = project_colors[3], alpha = 0.7, size = 1.2)
}
```

`plot_GP_simulation` takes as input what the first function delivers and provides a plot.

```{r}
get_GP_simulation(
  x = seq(-1,1, length.out = 500), 
  Intercept = 0, 
  slope = 0.3, 
  sigma_f = 0.05, 
  lambda  = 20, 
  sigma = 0.05) |> 
  plot_GP_simulation()
```

blue line is a regular, simple linear regression line, the *linear predictor* part, if you will. The red line is the *predictor of central tendency*, obtained by overlaying the linear predictor with “wiggles” sampled from the Gaussian process. The yellow dots are actual samples (obtained from a normal distribution) around the central predictor line.

This plot illustrates how to combine a simple linear trend with the extra flexibility of a Gaussian process. We will:

1.  Fit a straight‐line predictor\
2.  Sample a smooth “wiggle” from a zero‐mean Gaussian process\
3.  Add independent noise to generate synthetic observations

The final plot shows:

-   **Blue line**: the linear predictor (m(x) = \beta\_0 + \beta\_1 x).\
-   **Red line**: the latent function (f(x) = m(x) + g(x)), where (g(x)) is one draw from a GP.\
-   **Yellow dots**: observations (y_i \sim \mathcal{N}(f(x_i), \sigma\^2)).

# 
