---
title: "Simulation + EDA"
format: html
editor: visual
---

# Chapter 4: Exploratory Data Analysis and Simulation Design

To build a simulation that closely replicates the empirical characteristics of the gold-like closing price dataset, we performed exploratory data analysis (EDA) on the true historical closing price data provided in `FINAL_USO.csv`. The analysis was supported by summary statistics, autocorrelation diagnostics, and rolling volatility indicators.

### 4.1 Closing Price Dynamics

```{r, fig.cap="Figure 4.1: Daily gold-like closing price (USD). The series shows a downward trend between 2012–2015, followed by sideways fluctuations with irregular volatility."}

library(tidyverse)
library(zoo)
library(lubridate)

# Load the data
df <- read_csv("FINAL_USO.csv") %>%
  mutate(Date = as.Date(Date)) %>%
  arrange(Date)

# Plot closing price
ggplot(df, aes(x = Date, y = Close)) +
  geom_line(color = "goldenrod", size = 0.5) +
  labs(
    title = "Gold-Like Closing Price (USD)",
    x = "Date", y = "Close Price (USD)"
  ) +
  theme_minimal(base_size = 14)

```

The series exhibits a downward trend between 2012 and 2016, followed by sideways fluctuations with irregular volatility. Importantly, there is no strong seasonal structure, and the series instead reflects regime shifts and bursts of volatility.

Recommendation: This behavior can be modeled with a piecewise deterministic drift or linearly declining trend to capture regime-dependent dynamics, a practice consistent with established time-series methodology (Hamilton, 1994).

### 4.2 Descriptive Statistics

```{r,fig.cap="Table 4.1: Descriptive statistics of the closing price series. Prices range mostly between $100 and $175, with a mean around $127, confirming bounded but non-stationary behavior."}
summary(df$Close)

```

On average, the closing price fluctuates between \$100 and \$175, with a long-run mean of approximately \$127. This confirms that the data are non-stationary, but still bounded within realistic market ranges for our selected time-frame.

### 4.3 Return Dynamics

```{r,fig.cap="Figure 4.2: Autocorrelation function (ACF) of log returns. Correlations are close to zero across lags, indicating weak serial dependence in returns."}

# Calculate log returns
df <- df %>%
  mutate(Returns = log(Close / lag(Close)))

# Plot autocorrelation of returns
acf(na.omit(df$Returns), main = "ACF of Log Returns")

```

The autocorrelation of log returns is very close to zero (-0.0688 at lag 1), suggesting returns are largely uncorrelated. This finding is consistent with the stylized facts of financial time series, where returns often behave like a noisy or random process (Cont, 2001).

Recommendation: Simulation can begin from a Geometric Brownian Motion (GBM) framework, while extending it with heteroskedastic components to capture volatility dynamics.

### 4.4 Rolling Mean and Volatility

```{r, fig.cap="Figure 4.3: Rolling 100-day mean (left) and rolling 100-day volatility (right). The rolling mean highlights long-term shifts in drift, while the rolling volatility shows alternating calm and turbulent periods."}
library(gridExtra)
# Compute rolling mean and volatility
df <- df %>%
  mutate(
    RollingMean = rollmean(Close, k = 100, fill = NA, align = "right"),
    RollingVol  = rollapply(Close, width = 100, FUN = sd, fill = NA, align = "right")
  )

# Rolling Mean plot
p1 <- ggplot(na.omit(df), aes(x = Date)) +
  geom_line(aes(y = Close), color = "gray", alpha = 0.4) +
  geom_line(aes(y = RollingMean), color = "blue", size = 1) +
  labs(title = "Rolling 100-Day Mean", y = "Price (USD)", x = "Date") +
  theme_minimal(base_size = 12)

# Rolling Volatility plot
p2 <- ggplot(na.omit(df), aes(x = Date, y = RollingVol)) +
  geom_line(color = "red", size = 1) +
  labs(title = "Rolling 100-Day Volatility", y = "Volatility", x = "Date") +
  theme_minimal(base_size = 12)

# Arrange side by side
grid.arrange(p1, p2, ncol = 2)

```

The rolling mean illustrates shifts in long-term drift, while the rolling standard deviation highlights volatility clustering — extended periods of calm alternating with periods of turbulence.

Recommendation: Such dynamics can be simulated using models that allow for time-varying variance, such as GARCH processes (Engle, 1982). This ensures the simulated series reflects the empirical volatility bursts observed in the data.

### 4.5 Summary of Findings

The EDA provides clear evidence of the following features:

Downward Drift (2012–2015):A piecewise deterministic trend should be included to replicate early decline (Hamilton, 1994).

Volatility Clustering: Heteroskedastic processes such as GARCH are necessary to model alternating calm and turbulent periods (Engle, 1982).

No Seasonality: The absence of seasonal effects supports excluding sine/cosine terms or seasonal ARIMA models (Cont, 2001).

Irregular Noise: Returns exhibit noisy, uncorrelated behavior, motivating stochastic simulation with mean-reverting adjustments (Rasmussen & Williams, 2006).

Together, these insights form the foundation for designing a simulation process that faithfully mirrors the empirical structure of the gold-like dataset, before applying Gaussian Process regression and other time-series forecasting methods.

###4.6 Simulation of a Gold-Like Time Series

We simulate a synthetic series that shares key properties of the real dataset. The simulation includes:

1.  Deterministic downward drift (linear decay in the early part).

2.  Stochastic noise with volatility bursts (to mimic clustering).

3.  No seasonal cycles (confirmed in EDA).

```{r, fig.cap="Figure 4.4: Simulated gold-like closing price series with downward drift, volatility clustering, and irregular fluctuations.", out.width="100%"}
library(ggplot2)
set.seed(123)

n <- nrow(df)             # same length as real data
time <- 1:n

# Piecewise deterministic trend: early downward, then flat
trend <- ifelse(time < 800,
                170 - 0.05 * time,    # downward drift
                130)                  # stabilized level

# Volatility clustering: alternate calm & turbulent regimes
vol <- rep(c(2, 6), length.out = n)   # switching volatility regimes

# Random noise with bursts
noise <- rnorm(n, mean = 0, sd = vol)

# Add weak mean-reversion to keep values bounded
sim_price <- trend + cumsum(noise * 0.2)

# Convert to data frame for plotting
sim_df <- data.frame(Date = df$Date, Sim_Close = sim_price)

# Plot
ggplot(sim_df, aes(x = Date, y = Sim_Close)) +
  geom_line(color = "darkgreen", size = 0.6) +
  labs(
    title = "Simulated Gold-Like Closing Price",
    x = "Date", y = "Simulated Close Price (USD)"
  ) +
  theme_minimal(base_size = 14)

```

The simulation produces a synthetic gold-like series of the same length as the real dataset. The first 800 days replicate the downward drift observed from 2012 to 2015. Thereafter, the process stabilizes around a constant level, reflecting the sideways dynamics from 2016 onward. Volatility clustering is incorporated by alternating between high- and low-variance noise regimes, which produces extended calm periods followed by turbulent fluctuations. Finally, a weak mean-reversion term ensures the process remains bounded between approximately 100 and 170 USD.

This simulated dataset will serve as a controlled environment for testing Gaussian Process models and comparing them against benchmark time-series methods.

```{r}
#Check mean prediction
library(kernlab)

# Prepare data for GP regression
x <- as.matrix(time)  # time index as input
y <- sim_price        # simulated closing prices

# Fit GP model (RBF kernel)
gp_model <- gausspr(x, y, kernel = "rbfdot")

# Predict (including on a test set if you wish)
y_pred <- predict(gp_model, x)

# Plot actual vs GP fit
plot(x, y, type = "l", col = "darkgreen", lwd = 2, main = "GP Fit to Simulated Gold Price")
lines(x, y_pred, col = "blue", lwd = 2)
legend("topleft", legend = c("Simulated", "GP fit"), col = c("darkgreen", "blue"), lty = 1, lwd = 2)

```

\
Bayesian Approach (Rstan)

```{r}


x <- sim_df$Date
y <- sim_df$Sim_Close

# It's strongly recommended to scale x and y for Stan numerical stability:
x_scaled <- as.numeric(scale(x))
y_scaled <- as.numeric(scale(y))
N <- length(x_scaled)

```

```{r}
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

stan_data <- list(
  N = length(x),
  x = as.numeric(x),  # not as.matrix, not as.vector (if it's a matrix/column, use drop = TRUE)
  y = as.numeric(y)
)


fit <- stan(
  file = "gp_rbf.stan",
  data = stan_data,
  iter = 1000,
  chains = 4,
  seed = 42
)

print(fit, pars = c("alpha", "rho", "sigma"))

```

Marginal likelihood approach

```{r}
library(tidyverse)
library(kernlab)

# Simulate as before
set.seed(123)
df <- read_csv("FINAL_USO.csv") %>%
  mutate(Date = as.Date(Date)) %>%
  arrange(Date)
n <- nrow(df)
time <- 1:n

trend <- ifelse(time < 800,
                170 - 0.05 * time,
                130)
vol <- rep(c(2, 6), length.out = n)
noise <- rnorm(n, mean = 0, sd = vol)
sim_price <- trend + cumsum(noise * 0.2)
sim_df <- data.frame(Date = df$Date, Sim_Close = sim_price

```

```{r}
library(tidyverse)
library(kernlab)


set.seed(123)
df <- read_csv("FINAL_USO.csv") %>%
  mutate(Date = as.Date(Date)) %>%
  arrange(Date)
n <- nrow(df)
time <- 1:n

trend <- ifelse(time < 800, 170 - 0.05 * time, 130)
vol <- rep(c(2, 6), length.out = n)
noise <- rnorm(n, mean = 0, sd = vol)
sim_price <- trend + cumsum(noise * 0.2)
sim_df <- data.frame(Time = time, Date = df$Date, Sim_Close = sim_price)

#  Fit a linear (drift) mean to the data 
lin_fit <- lm(Sim_Close ~ Time, data = sim_df)
mean_pred <- predict(lin_fit, newdata = sim_df)
resid_gp <- sim_df$Sim_Close - mean_pred

# Fit GP to the residuals 
x_mat <- as.matrix(sim_df$Time)
gp_fit <- gausspr(x = x_mat, y = resid_gp, kernel = "rbfdot")  # RBF kernel

#  Predict on all data (add mean function back)
gp_resid_pred <- predict(gp_fit, x_mat)
gp_total_pred <- mean_pred + gp_resid_pred

# Simulated series, drift fit, and GP+drift fit
library(ggplot2)
sim_df <- sim_df %>%
  mutate(
    Drift_Fit = mean_pred,
    GP_Drift_Fit = gp_total_pred
  )

ggplot(sim_df, aes(x = Date)) +
  geom_line(aes(y = Sim_Close), color = "darkgreen", size = 0.8, alpha = 0.7, linetype = "solid") +
  geom_line(aes(y = Drift_Fit), color = "blue", size = 1, linetype = "dashed") +
  geom_line(aes(y = GP_Drift_Fit), color = "red", size = 1.1, linetype = "solid") +
  labs(
    title = "Simulated Gold Price: True vs Drift Mean vs GP+Drift",
    y = "Simulated Close Price (USD)",
    x = "Date"
  ) +
  scale_color_manual(
    values = c("Simulated" = "darkgreen", "Drift Mean" = "blue", "GP+Drift" = "red")
  ) +
  theme_minimal(base_size = 14)



```

```{r}
library(tidyverse)
library(kernlab)
library(forecast)


h <- 30
train_idx <- 1:(n - h)
test_idx  <- (n - h + 1):n
train <- sim_df[train_idx, ]
test  <- sim_df[test_idx, ]

# Mean model
mean_pred <- rep(mean(train$Sim_Close), h)

# Naive model (last observed value) 
naive_pred <- rep(train$Sim_Close[nrow(train)], h)

#Seasonal naive (1 year = 252 days) 
seasonal_period <- 252
seasonal_naive_pred <- if (nrow(train) >= seasonal_period) {
  train$Sim_Close[(nrow(train) - seasonal_period + 1):(nrow(train) - seasonal_period + h)]
} else {
  rep(mean(train$Sim_Close), h) 
}

# Drift model 
drift_mod <- rwf(train$Sim_Close, h = h, drift = TRUE)
drift_pred <- as.numeric(drift_mod$mean)

# AR(1) model 
ar_mod <- Arima(train$Sim_Close, order = c(1,0,0))
ar_pred <- as.numeric(forecast(ar_mod, h = h)$mean)

# GP + Drift , note the drift is fitted to training data
lin_fit <- lm(Sim_Close ~ Time, data = train)
drift_train <- predict(lin_fit, newdata = train)
drift_test  <- predict(lin_fit, newdata = test)
resid_gp <- train$Sim_Close - drift_train

# GP on training residuals
x_mat <- as.matrix(train$Time)
gp_fit <- gausspr(x = x_mat, y = resid_gp, kernel = "rbfdot")
gp_pred_train <- predict(gp_fit, x_mat)
gp_pred_test  <- predict(gp_fit, as.matrix(test$Time))

# GP+Drift prediction
gpdrift_pred <- drift_test + gp_pred_test

# --- Metrics function ---
metric_fun <- function(true, pred) {
  rmse <- sqrt(mean((true - pred)^2))
  mae  <- mean(abs(true - pred))
  mape <- mean(abs((true - pred) / true)) * 100
  c(RMSE = rmse, MAE = mae, MAPE = mape)
}

results <- tibble(
  Model  = c("Mean", "Naive", "Seasonal Naive", "Drift", "AR(1)", "GP+Drift"),
  RMSE   = c(
    metric_fun(test$Sim_Close, mean_pred)[1],
    metric_fun(test$Sim_Close, naive_pred)[1],
    metric_fun(test$Sim_Close, seasonal_naive_pred)[1],
    metric_fun(test$Sim_Close, drift_pred)[1],
    metric_fun(test$Sim_Close, ar_pred)[1],
    metric_fun(test$Sim_Close, gpdrift_pred)[1]
  ),
  MAE    = c(
    metric_fun(test$Sim_Close, mean_pred)[2],
    metric_fun(test$Sim_Close, naive_pred)[2],
    metric_fun(test$Sim_Close, seasonal_naive_pred)[2],
    metric_fun(test$Sim_Close, drift_pred)[2],
    metric_fun(test$Sim_Close, ar_pred)[2],
    metric_fun(test$Sim_Close, gpdrift_pred)[2]
  ),
  MAPE   = c(
    metric_fun(test$Sim_Close, mean_pred)[3],
    metric_fun(test$Sim_Close, naive_pred)[3],
    metric_fun(test$Sim_Close, seasonal_naive_pred)[3],
    metric_fun(test$Sim_Close, drift_pred)[3],
    metric_fun(test$Sim_Close, ar_pred)[3],
    metric_fun(test$Sim_Close, gpdrift_pred)[3]
  )
)

print(results, n = Inf)

```

try GP-0-RBF next

cmdstanr:

```{r}
library(cmdstanr)
library(posterior)
library(bayesplot)

sim_df$Time <- as.numeric(sim_df$Date - min(sim_df$Date)) + 1
X <- matrix(sim_df$Time, ncol = 1)
y <- sim_df$Sim_Close
n <- nrow(sim_df)

```

```{r}
library(cmdstanr)
stan_data <- list(N = n, x = as.vector(X), y = y)

mod <- cmdstan_model("gp_regression.stan")
fit <- mod$sample(
  data = stan_data,
  seed = 2024,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000,
  iter_sampling = 1000,
  refresh = 500
)

```

### 4.7 Gaussian Process Modeling of Simulated Data

Following the exploratory analysis and simulation, we now fit a Gaussian Process (GP) model to the simulated gold-like closing price series. This follows the robust workflow outlined in Betancourt’s Robust Gaussian Process Modeling tutorial, which stresses:

1.  Careful data preprocessing (centering and scaling).
2.  Explicit kernel construction with priors on hyperparameters.
3.  Robust inference using cmdstanr.
4.  Posterior predictive checks with uncertainty quantification.

### 4.7.1 Data Preparation

Gaussian Processes assume stationarity in their kernels. Since our simulated series contains large offsets and long-term drift (identified in Section 4.6), we first center and scale the series to stabilize inference.

```{r}
library(cmdstanr)

# Stand. sim price
y <- scale(sim_df$Sim_Close, center = TRUE, scale = TRUE)
x <- 1:length(y)

gp_data <- list(
  N = length(y),
  x = as.vector(x),
  y = as.vector(y)
)
```

### 4.7.2 Stan Model

We adopt the squared exponential (RBF) kernel, written explicitly as in Betancourt’s tutorial. Importantly, we parameterize hyperparameters on the log scale for stability, and use weakly informative priors to avoid pathological fits.

### 4.7.3 Model Fitting and Diagnostics

```{r}

mod <- cmdstan_model("gp_model.stan")


fit <- mod$sample(
  data = gp_data,
  seed = 2025,
  chains = 4,
  parallel_chains = 4,
  iter_sampling = 1000,
  iter_warmup = 1000,
  refresh = 500
)
```

### 4.7.4 Hyperparameter Analysis

```{r}
library(bayesplot)

draws <- gp_fit$draws(format = "df")

# Hyperparameter posterior dist


mcmc_areas(draws, pars = c("rho", "alpha", "sigma")) +
  ggtitle("Posterior Distributions of GP Hyperparameters") +
  theme_minimal()
```

The length scale parameter rho controls the smoothness of the GP function - larger values indicate smoother functions. The amplitude parameter alpha controls the marginal standard deviation of the process. The noise parameter sigma captures measurement error.

### 4.7.5 Posterior Predictive Checks

Following Betancourt's emphasis on model validation, we perform posterior predictive checks:

```{r}
# Extract posterior predictive samples
y_rep <- draws %>% 
  select(starts_with("f_rep")) %>%
  as.matrix()

# Posterior predictive check
ppc_dens_overlay(gp_data$y, y_rep[1:100, ]) +
  ggtitle("Posterior Predictive Check: Density Overlay") +
  theme_minimal()


ppc_stat(gp_data$y, y_rep, stat = "mean") +
  ggtitle("PPC: Mean")

ppc_stat(gp_data$y, y_rep, stat = "sd") +
  ggtitle("PPC: Standard Deviation")
```

```{r}

library(forecast)
library(tidyverse)


sim_ts <- ts(sim_df$Sim_Close, frequency = 1)  # No seasonality, daily data

# Split data into train/test (e.g., last 100 points for testing)
n <- length(sim_ts)
train <- window(sim_ts, end = n - 100)
test <- window(sim_ts, start = n - 99)

# Mean Forecast (Average)
mean_fit <- meanf(train, h = 100)

# Naive Forecast
naive_fit <- naive(train, h = 100)

#  Drift Forecast
drift_fit <- rwf(train, h = 100, drift = TRUE)

# ARIMA Forecast , use AR(1) INSTEAD 
arima_fit <- auto.arima(train)
arima_forecast <- forecast(arima_fit, h = 100)

# GP Forecast

forecasts <- tibble(
  Actual = as.numeric(test),
  Mean = as.numeric(mean_fit$mean),
  Naive = as.numeric(naive_fit$mean),
  Drift = as.numeric(drift_fit$mean),
  ARIMA = as.numeric(arima_forecast$mean)
)

# Evaluate performance
evaluate <- function(pred, actual) {
  tibble(
    RMSE = sqrt(mean((actual - pred)^2)),
    MAE = mean(abs(actual - pred)),
    R2 = cor(actual, pred)^2
  )
}

# Create performance table
performance <- bind_rows(
  evaluate(forecasts$Mean, forecasts$Actual),
  evaluate(forecasts$Naive, forecasts$Actual),
  evaluate(forecasts$Drift, forecasts$Actual),
  evaluate(forecasts$ARIMA, forecasts$Actual)
)

performance <- cbind(Model = c("Mean", "Naive", "Drift", "ARIMA"), performance)

# performance table
print(performance)
```

llllllllllll

```{r}
sim_df <- read_csv("simulated_gold_prices.csv")
sim_df$Time <- as.numeric(sim_df$Date - min(as.Date(sim_df$Date))) + 1

n <- nrow(sim_df)
train_idx <- 1:(n - 30)      # all but last 30
test_idx <- (n - 29):n       # last 30 for forecast

X_train <- matrix(sim_df$Time[train_idx], ncol = 1)
y_train <- sim_df$Sim_Close[train_idx]
X_test  <- matrix(sim_df$Time[test_idx], ncol = 1)
y_test  <- sim_df$Sim_Close[test_idx]

# RBF kernel (default)
gp_model <- gausspr(
  x = X_train,
  y = y_train,
  kernel = "rbfdot"
)


```

```{r}
# In-sample fit
gp_fit <- predict(gp_model, X_train)

# Forecast (out-of-sample)
gp_forecast <- predict(gp_model, X_test)

```

```{r}
# Plot full fit and forecast
plot(sim_df$Time, sim_df$Sim_Close, type = "l", col = "darkgreen", lwd = 2,
     xlab = "Time", ylab = "Simulated Close Price", main = "GP Fit and Forecast")

lines(sim_df$Time[train_idx], gp_fit, col = "blue", lwd = 2)
lines(sim_df$Time[test_idx], gp_forecast, col = "red", lwd = 2)

abline(v = sim_df$Time[n - 30], col = "gray", lty = 2)
legend("topleft", legend = c("Simulated", "GP Fit", "GP Forecast"),
       col = c("darkgreen", "blue", "red"), lty = 1, lwd = 2)

```

\

```{r}
mse  <- mean((y_test - gp_forecast)^2)
mae  <- mean(abs(y_test - gp_forecast))
mape <- mean(abs((y_test - gp_forecast) / y_test)) * 100

cat(sprintf("Test MSE: %.3f, MAE: %.3f, MAPE: %.2f%%\n", mse, mae, mape))

```

```{r}
library(cmdstanr)

# Scale the time variable (important for GPs!)
sim_df$Time_scaled <- as.numeric(scale(as.numeric(sim_df$Date)))
N <- nrow(sim_df)

# Prepare data
stan_data <- list(
  N = N,
  x = sim_df$Time_scaled,
  y = sim_df$Sim_Close
)

# Compile the Stan model
mod <- cmdstan_model("gp_drift_model.stan")

# Fit the model
fit <- mod$sample(
  data = stan_data,
  seed = 123,
  chains = 4,
  iter_warmup = 500,
  iter_sampling = 1000,
  parallel_chains = 4,
  refresh = 100
)

# Extract posterior summaries
fit$print()

# Extract posterior draws for prediction
posterior <- fit$draws()
beta0_hat <- mean(as.numeric(posterior[,"beta0"]))
beta1_hat <- mean(as.numeric(posterior[,"beta1"]))

# Posterior mean for drift
mu_pred <- beta0_hat + beta1_hat * sim_df$Time_scaled

# Plot (base R or ggplot2)
plot(sim_df$Date, sim_df$Sim_Close, type = "l", col = "darkgreen", lwd = 2,
     main = "Stan GP with Drift (cmdstanr)", xlab = "Date", ylab = "Simulated Close Price")
lines(sim_df$Date, mu_pred, col = "blue", lwd = 2)
legend("topleft", legend = c("Simulated", "Posterior Mean (Drift+GP)"),
       col = c("darkgreen", "blue"), lwd = 2)

```

ASSUMPTION THAT ERRORS \~N(0, sigma\^2)

```{r}
#--- Packages ---
library(mvtnorm)
library(ggplot2)

#--- Define kernel ---
sqdist <- function(x, xp) outer(x, xp, function(a,b) (a-b)^2)
cov_exp_quad <- function(x, xp, alpha, rho) alpha^2 * exp(-0.5 * sqdist(x,xp)/rho^2)

#--- Data setup ---
y <- sim_df$Sim_Close
x <- as.numeric(scale(1:length(y)))  # scaled time for numerical stability

#--- Hyperparameters (rough guesses or estimates) ---
alpha <- sd(y) * 0.5   # signal amplitude
rho   <- 0.3           # smoothness
sigma <- sd(y - mean(y)) * 0.2  # noise level

#--- Compute GP posterior ---
K_xx <- cov_exp_quad(x, x, alpha, rho)
K_yy <- K_xx + diag(sigma^2, length(y))

# Mean-centering the data
y_centered <- y - mean(y)

# Compute Cholesky for stability
L <- chol(K_yy)
alpha_vec <- backsolve(L, forwardsolve(t(L), y_centered))

# Posterior mean and variance
post_mu <- K_xx %*% alpha_vec
V <- backsolve(L, t(K_xx))
post_cov <- K_xx - t(V) %*% V
post_sd <- sqrt(diag(post_cov))

#--- Combine results ---
gp_df <- data.frame(
  Date = sim_df$Date,
  y = y,
  mu = post_mu + mean(y),
  lo = post_mu + mean(y) - 2 * post_sd,
  hi = post_mu + mean(y) + 2 * post_sd
)

#--- Plot GP posterior ---
ggplot(gp_df, aes(x = Date)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "skyblue", alpha = 0.4) +
  geom_line(aes(y = mu), color = "blue", size = 0.7) +
  geom_line(aes(y = y), color = "darkgreen", alpha = 0.5) +
  labs(
    title = "Gaussian Process Fit to Simulated Gold-like Price Series",
    subtitle = "Posterior mean (blue) with 95% credible band (shaded)",
    x = "Date", y = "Price (USD)"
  ) +
  theme_minimal(base_size = 14)

```

```{r}
gp_zoom <- subset(gp_df, Date >= as.Date("2014-01-01") & Date <= as.Date("2016-12-31"))

ggplot(gp_zoom, aes(x = Date)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "skyblue", alpha = 0.4) +
  geom_line(aes(y = mu), color = "blue", size = 0.7) +
  geom_line(aes(y = y), color = "darkgreen", alpha = 0.5) +
  labs(
    title = "Gaussian Process Fit (Zoomed on 2014–2016)",
    subtitle = "Posterior mean (blue) with 95% credible band (shaded)",
    x = "Date", y = "Price (USD)"
  ) +
  theme_minimal(base_size = 14)


```

```{r}
# ---- setup ----
library(mvtnorm)
library(ggplot2)

y  <- sim_df$Sim_Close
x0 <- 1:length(y)
x  <- as.numeric(scale(x0))           # scale time to ~[-1,1] for stability
n  <- length(y)

sqdist <- function(x, xp) outer(x, xp, function(a,b) (a-b)^2)
cov_exp_quad <- function(x, xp, alpha, rho) alpha^2 * exp(-0.5 * sqdist(x,xp)/rho^2)

# ---- negative log marginal likelihood (NLML) ----
neg_log_marglik <- function(par_log, x, y) {
  alpha <- exp(par_log[1]); rho <- exp(par_log[2]); sigma <- exp(par_log[3])
  K <- cov_exp_quad(x, x, alpha, rho) + diag(sigma^2 + 1e-10, length(x))
  # mean-center (constant mean); you can replace with a trend if desired
  y_c <- y - mean(y)
  L   <- chol(K)
  v   <- backsolve(L, y_c, transpose = TRUE)
  0.5*sum(v^2) + sum(log(diag(L))) + 0.5*length(y)*log(2*pi)
}

# ---- multi-start optimization (to avoid bad local optima) ----
set.seed(1)
starts <- rbind(
  log(c(sd(y)*0.3,  0.20, sd(y)*0.2)),
  log(c(sd(y)*0.7,  0.40, sd(y)*0.3)),
  log(c(sd(y)*1.2,  0.80, sd(y)*0.5)),
  replicate(3, log(c(runif(1, 0.1, 2)*sd(y), runif(1, 0.05, 1.5), runif(1, 0.05, 1.5)*sd(y))), simplify=TRUE)
)

opts <- apply(starts, 2, function(s0)
  optim(s0, neg_log_marglik, x = x, y = y,
        method = "L-BFGS-B",
        lower = log(c(1e-6, 1e-4, 1e-6)),
        upper = log(c(1e6,  5.0,  1e6)))
)

best <- opts[[ which.min(sapply(opts, `[[`, "value")) ]]
theta_hat <- setNames(exp(best$par), c("alpha","rho","sigma"))
theta_hat

```

```{r}
# ---- posterior with estimated hyper-parameters ----
alpha_hat <- theta_hat["alpha"]; rho_hat <- theta_hat["rho"]; sigma_hat <- theta_hat["sigma"]

Kxx <- cov_exp_quad(x, x, alpha_hat, rho_hat)
Kyy <- Kxx + diag(sigma_hat^2 + 1e-10, n)

y_c <- y - mean(y)
L   <- chol(Kyy)
a   <- backsolve(L, forwardsolve(t(L), y_c))

post_mu  <- Kxx %*% a
V        <- backsolve(L, t(Kxx))
post_cov <- Kxx - t(V) %*% V
post_sd  <- sqrt(pmax(0, diag(post_cov)))   # guard tiny negatives

gp_fit <- data.frame(
  Date = sim_df$Date,
  y    = y,
  mu   = as.numeric(post_mu + mean(y)),
  lo   = as.numeric(post_mu + mean(y) - 1.96*post_sd),
  hi   = as.numeric(post_mu + mean(y) + 1.96*post_sd)
)

# ---- plot: GP fit with learned hyper-parameters ----
ggplot(gp_fit, aes(Date)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "skyblue", alpha = 0.8) +
  geom_line(aes(y = mu), color = "royalblue", linewidth = 0.6) +
  geom_line(aes(y = y), color = "darkgreen", alpha = 0.35) +
  labs(
    title = "GP Fit to Simulated Gold-like Series (Hyper-parameters Learned by Marginal Likelihood)",
    subtitle = paste0("alpha=", round(alpha_hat,2),
                      ", rho=", round(rho_hat,3),
                      ", sigma=", round(sigma_hat,2)),
    x = "Date", y = "Price (USD)"
  ) +
  theme_minimal(base_size = 14)

```

```{r}
# ---- subset for zoom (e.g., 2014–2016) ----
gp_zoom <- subset(gp_fit, Date >= as.Date("2014-01-01") & Date <= as.Date("2016-12-31"))

# ---- plot zoomed GP fit ----
ggplot(gp_zoom, aes(Date)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), fill = "skyblue", alpha = 0.6) +
  geom_line(aes(y = mu), color = "royalblue", linewidth = 0.6) +
  geom_line(aes(y = y), color = "darkgreen", alpha = 0.4) +
  labs(
    title = "Zoomed GP Fit to Simulated Gold-like Series (2014–2016)",
    subtitle = paste0("α = ", round(alpha_hat,2),
                      ", ρ = ", round(rho_hat,3),
                      ", σ = ", round(sigma_hat,2)),
    x = "Date", y = "Price (USD)"
  ) +
  theme_minimal(base_size = 15)
```

something to think about :) ![](images/clipboard-798855471.png)

\
\

```{r}
library(ggplot2)

#------------------------
# 1) Train / Test split
#------------------------
y_all <- sim_df$Sim_Close
dates <- sim_df$Date
n <- length(y_all); h <- 30
idx_tr <- 1:(n - h); idx_te <- (n-h+1):n
y_tr <- y_all[idx_tr]; y_te <- y_all[idx_te]
d_tr <- dates[idx_tr]; d_te <- dates[idx_te]

#------------------------
# 2) Helpers
#------------------------
rmse <- function(e) sqrt(mean(e^2))
mae  <- function(e) mean(abs(e))
mape <- function(y, yhat) mean(abs((y - yhat)/y)) * 100

eval_metrics <- function(y_true, y_hat) {
  e <- y_true - y_hat
  data.frame(IRMSE = rmse(e), IMAE = mae(e), IMPE = mape(y_true, y_hat))
}

#------------------------
# 3) Baseline forecasts
#------------------------

# Mean: constant = mean(train)
f_mean <- rep(mean(y_tr), h)

# Naive: last observed
f_naive <- rep(tail(y_tr, 1), h)

# Seasonal naive (m = 7 day-of-week seasonality; adjust m if needed)
m <- 7
last_season <- tail(y_tr, m)
# repeat last_season to length h, aligned
f_snaive <- rep(last_season, length.out = h)

# Drift: line between first and last, extrapolate
y1 <- y_tr[1]; yT <- tail(y_tr, 1); Tn <- length(y_tr)
drift_step <- (yT - y1) / (Tn - 1)
f_drift <- yT + drift_step * (1:h)

# AR(1): ARIMA(1,0,0)
fit_ar1 <- arima(y_tr, order = c(1,0,0), include.mean = TRUE)
f_ar1 <- as.numeric(predict(fit_ar1, n.ahead = h)$pred)

#------------------------
# 4) GP forecast (RBF kernel, hyperparams by ML on train only)
#------------------------
sqdist <- function(x, xp) outer(x, xp, function(a,b) (a-b)^2)
cov_exp_quad <- function(x, xp, alpha, rho) alpha^2 * exp(-0.5 * sqdist(x,xp)/rho^2)

x_tr0 <- 1:length(y_tr)
x_te0 <- (length(y_tr)+1):(length(y_tr)+h)
# scale to ~[-1,1] for stability
scale01 <- function(z){ (z - mean(x_tr0)) / sd(x_tr0) }
x_tr <- scale01(x_tr0)
x_te <- scale01(x_te0)

neg_log_marglik <- function(par_log, x, y) {
  alpha <- exp(par_log[1]); rho <- exp(par_log[2]); sigma <- exp(par_log[3])
  K <- cov_exp_quad(x, x, alpha, rho) + diag(sigma^2 + 1e-10, length(x))
  y_c <- y - mean(y)
  L <- chol(K)
  v <- backsolve(L, y_c, transpose = TRUE)
  0.5*sum(v^2) + sum(log(diag(L))) + 0.5*length(y)*log(2*pi)
}

# multi-start to reduce local minima risk
set.seed(123)
starts <- rbind(
  log(c(sd(y_tr)*0.3, 0.2, sd(y_tr)*0.3)),
  log(c(sd(y_tr)*1.0, 0.6, sd(y_tr)*0.5)),
  log(c(sd(y_tr)*2.0, 1.0, sd(y_tr)*0.8))
)
opts <- apply(starts, 1, function(s0)
  optim(s0, neg_log_marglik, x = x_tr, y = y_tr, method = "L-BFGS-B",
        lower = log(c(1e-6, 1e-4, 1e-6)),
        upper = log(c(1e6,  5.0,  1e6))))
best <- opts[[ which.min(sapply(opts, `[[`, "value")) ]]
theta_hat <- setNames(exp(best$par), c("alpha","rho","sigma"))

alpha_hat <- theta_hat["alpha"]; rho_hat <- theta_hat["rho"]; sigma_hat <- theta_hat["sigma"]

# GP predictive mean for future x_te (posterior predictive mean)
Koo <- cov_exp_quad(x_tr, x_tr, alpha_hat, rho_hat) + diag(sigma_hat^2 + 1e-10, length(x_tr))
Kto <- cov_exp_quad(x_te, x_tr, alpha_hat, rho_hat)
L   <- chol(Koo)
a   <- backsolve(L, forwardsolve(t(L), y_tr - mean(y_tr)))
mu_te <- as.numeric(Kto %*% a + mean(y_tr))

# (If you also want predictive intervals, add sigma^2, but table uses point forecasts)

#------------------------
# 5) Metric table (IRMSE/IMAE/IMPE on 30-day holdout)
#------------------------
tab <- rbind(
  Mean           = eval_metrics(y_te, f_mean),
  Naive          = eval_metrics(y_te, f_naive),
  SeasonalNaive  = eval_metrics(y_te, f_snaive),
  Drift          = eval_metrics(y_te, f_drift),
  AR1            = eval_metrics(y_te, f_ar1),
  GP_RBF         = eval_metrics(y_te, mu_te)   
)

round(tab, 3)

```

maybe a composite kernel (Linear + Weekly Periodic + Matérn-3/2)

```{r}
# ---- helpers (add to your GP section) ----
sqdist  <- function(x, xp) outer(x, xp, function(a,b) (a-b)^2)
absdist <- function(x, xp) abs(outer(x, xp, "-"))

# Linear kernel: k(x,x') = (c + s*(x-cx))*(c + s*(x'-cx))  ~ allows linear trend extrapolation
cov_linear <- function(x, xp, bias, slope, cx) {
  xv  <- (x - cx); xpv <- (xp - cx)
  (bias + slope * xv) %o% (bias + slope * xpv)
}

# Weekly periodic kernel (period P): k = a^2 * exp(-2 sin^2(pi|x-x'|/P) / l^2)
cov_periodic <- function(x, xp, amp, ell, period) {
  r <- absdist(x, xp)
  amp^2 * exp( -2 * (sin(pi * r / period)^2) / (ell^2) )
}

# Matérn 3/2 (rough local structure)
cov_matern32 <- function(x, xp, alpha, rho) {
  r <- absdist(x, xp); a <- sqrt(3) * r / rho
  alpha^2 * (1 + a) * exp(-a)
}

# Sum kernel: Linear + Periodic(weekly) + Matérn32
cov_sum <- function(x, xp, par) {
  with(as.list(par), {
    cov_linear(x, xp, bias_lin, slope_lin, cx) +
    cov_periodic(x, xp, amp_per, ell_per, period) +
    cov_matern32(x, xp, alpha_m32, rho_m32)
  })
}

# Negative log marginal likelihood for composite kernel
nlml_sum <- function(p_log, x, y, period = 7) {
  p <- exp(p_log)
  names(p) <- c("bias_lin","slope_lin","amp_per","ell_per","alpha_m32","rho_m32","sigma")
  p <- within(as.list(p), { cx <- 0; period <- period })
  K <- cov_sum(x, x, p) + diag(p$sigma^2 + 1e-10, length(x))
  y_c <- y - mean(y)
  L <- chol(K)
  v <- backsolve(L, y_c, transpose = TRUE)
  0.5*sum(v^2) + sum(log(diag(L))) + 0.5*length(y)*log(2*pi)
}

gp_forecast_sum <- function(x_tr, y_tr, x_te, period = 7) {
  set.seed(42)
  init <- rbind(
    log(c(  1,  1e-2,  sd(y_tr)*0.3, 0.5, sd(y_tr)*0.8, 0.5, sd(y_tr)*0.3)),
    log(c( 10,  1e-3,  sd(y_tr)*0.6, 1.0, sd(y_tr)*1.2, 1.0, sd(y_tr)*0.6)),
    log(c(  3,  1e-2,  sd(y_tr)*1.0, 0.3, sd(y_tr)*0.5, 2.0, sd(y_tr)*0.4))
  )
  opts <- apply(init, 1, function(s0)
    optim(s0, nlml_sum, x = x_tr, y = y_tr, period = period,
          method="L-BFGS-B",
          lower = log(c(1e-6,1e-6,1e-6,1e-3,1e-6,1e-3,1e-6)),
          upper = log(c(1e+6,1e+2,1e+6, 5.0,1e+6, 5.0,1e+6))))
  best <- opts[[ which.min(sapply(opts, `[[`, "value")) ]]
  p <- exp(best$par); names(p) <- c("bias_lin","slope_lin","amp_per","ell_per","alpha_m32","rho_m32","sigma")
  p <- within(as.list(p), { cx <- 0; period <- period })

  Koo <- cov_sum(x_tr, x_tr, p) + diag(p$sigma^2 + 1e-10, length(x_tr))
  Kot <- cov_sum(x_te, x_tr, p)
  L <- chol(Koo)
  a <- backsolve(L, forwardsolve(t(L), y_tr - mean(y_tr)))
  mu_te <- as.numeric(Kot %*% a + mean(y_tr))
  list(mean = mu_te, par = p)
}

```

```{r}
# scale time as before
x_tr0 <- 1:length(y_tr)
x_te0 <- (length(y_tr)+1):(length(y_tr)+h)
scale01 <- function(z){ (z - mean(x_tr0)) / sd(x_tr0) }
x_tr <- scale01(x_tr0)
x_te <- scale01(x_te0)


# Composite (Linear + Weekly Periodic + Matérn32)
gp_combo <- gp_forecast_sum(x_tr, y_tr, x_te, period = 7)

f_gp_rbf   <- gp_rbf$mean
f_gp_m32   <- gp_m32$mean
f_gp_combo <- gp_combo$mean

```

```{r}
tab <- rbind(
  Mean           = eval_metrics(y_te, f_mean),
  Naive          = eval_metrics(y_te, f_naive),
  SeasonalNaive  = eval_metrics(y_te, f_snaive),
  Drift          = eval_metrics(y_te, f_drift),
  AR1            = eval_metrics(y_te, f_ar1),
  GP_RBF         = eval_metrics(y_te, f_gp_rbf),
  GP_Matern32    = eval_metrics(y_te, f_gp_m32),
  GP_Combo       = eval_metrics(y_te, f_gp_combo)
)
round(tab, 3)
```

```{r}
plot_df <- data.frame(
  Date = d_te,
  Actual = y_te,
  Mean = f_mean, Naive = f_naive, SNaive = f_snaive, Drift = f_drift, AR1 = f_ar1,
  GP_RBF = f_gp_rbf, GP_Matern = f_gp_m32, GP_Combo = f_gp_combo
) |>
  tidyr::pivot_longer(-c(Date, Actual), names_to="Model", values_to="Forecast")

ggplot(plot_df, aes(Date)) +
  geom_line(aes(y = Actual), color="black", linewidth=1.2) +
  geom_line(aes(y = Forecast, color = Model), linewidth=0.9, alpha=0.9) +
  scale_color_manual(values = c(
    Mean="gray40", Naive="orange", SNaive="pink", Drift="darkviolet",
    AR1="maroon", GP_RBF="blue", GP_Matern="green4", GP_Combo="red3"
  )) +
  labs(title="30-Day Forecasts (All Models)",
       subtitle="GP_Combo = Linear + Weekly Periodic + Matérn-3/2 kernel",
       y="Simulated Price (USD)", x="Date") +
  theme_minimal(base_size=14) +
  theme(legend.position="bottom")

```
