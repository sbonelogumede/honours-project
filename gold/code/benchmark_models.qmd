---
title: "full benchmark"
format: pdf
editor: visual
---

```{r}
# USO Oil Price Forecasting - COMPREHENSIVE BENCHMARKING
# Compares GP models against standard benchmarks
# Metrics: RMSE, MAE, MAPE for all models
# Dataset: FINAL_USO.csv
# Uses EXACT same methodology as uso_simple_robust.R and uso_ensemble_gpr.R

library(kernlab)
library(tidyverse)
library(zoo)
library(forecast)  # For ARIMA/AR models

set.seed(123)

# Set output directory (Windows path)
output_dir <- "C:/Users/Michaela/Documents/AZRRAP001/honours-project/gold/output"

# Create directory if it doesn't exist
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
  cat(sprintf("Created output directory: %s\n", output_dir))
}

cat("\n", rep("=", 80), "\n", sep = "")
cat("USO OIL PRICE FORECASTING - COMPREHENSIVE BENCHMARKING\n")
cat("Comparing GP Models vs Standard Benchmark Models\n")
cat(sprintf("Output directory: %s\n", output_dir))
cat(rep("=", 80), "\n\n", sep = "")
```

# ============================================================================

# 1. LOAD AND PREPARE DATA

# ============================================================================

```{r}
cat("Step 1: Loading data...\n")
data_path <- "FINAL_USO.csv"
raw_data <- read.csv(data_path, stringsAsFactors = FALSE)
raw_data$Date <- as.Date(raw_data$Date)

uso_data <- raw_data %>%
  select(Date, USO_Close) %>%
  rename(date = Date, price = USO_Close) %>%
  arrange(date) %>%
  filter(!is.na(price), is.finite(price))

cat(sprintf("Loaded %d observations\n", nrow(uso_data)))
cat(sprintf("Date range: %s to %s\n\n", min(uso_data$date), max(uso_data$date)))
```

# ============================================================================

# 2. TRAIN-TEST SPLIT (No feature engineering yet - use raw prices)

# ============================================================================

```{r}
cat("Step 2: Splitting data into train/test sets...\n")

n_total <- nrow(uso_data)
n_test <- round(n_total * 0.2)
split_index <- n_total - n_test

train_data <- uso_data[1:split_index, ]
test_data <- uso_data[(split_index + 1):n_total, ]

train_prices <- train_data$price
test_prices <- test_data$price
test_dates <- test_data$date

cat(sprintf("Train: %d observations | Test: %d observations\n\n", 
            nrow(train_data), nrow(test_data)))
```

# ============================================================================

# 3. BENCHMARK MODEL 1: MEAN

# ============================================================================

```{r}
cat("\n", rep("=", 80), "\n", sep = "")
cat("BENCHMARK 1: MEAN METHOD\n")
cat(rep("=", 80), "\n", sep = "")
cat("Predicts: Mean of all training data for every test point\n\n")

mean_forecast <- rep(mean(train_prices), n_test)

mean_error <- test_prices - mean_forecast
mean_rmse <- sqrt(mean(mean_error^2))
mean_mae <- mean(abs(mean_error))
mean_mape <- mean(abs(mean_error / test_prices)) * 100

cat(sprintf("  RMSE: $%.4f\n", mean_rmse))
cat(sprintf("  MAE:  $%.4f\n", mean_mae))
cat(sprintf("  MAPE: %.2f%%\n\n", mean_mape))

# ============================================================================
# 4. BENCHMARK MODEL 2: NAIVE (Random Walk)
# ============================================================================

cat(rep("=", 80), "\n", sep = "")
cat("BENCHMARK 2: NAIVE METHOD (Random Walk)\n")
cat(rep("=", 80), "\n", sep = "")
cat("Predicts: Last observed value (yesterday's price)\n\n")

# For each test point, use the previous observed value
naive_forecast <- numeric(n_test)
naive_forecast[1] <- train_prices[length(train_prices)]  # Last training value
for (i in 2:n_test) {
  naive_forecast[i] <- test_prices[i-1]  # Previous test value
}

naive_error <- test_prices - naive_forecast
naive_rmse <- sqrt(mean(naive_error^2))
naive_mae <- mean(abs(naive_error))
naive_mape <- mean(abs(naive_error / test_prices)) * 100

cat(sprintf("  RMSE: $%.4f\n", naive_rmse))
cat(sprintf("  MAE:  $%.4f\n", naive_mae))
cat(sprintf("  MAPE: %.2f%%\n\n", naive_mape))

# ============================================================================
# 5. BENCHMARK MODEL 3: SEASONAL NAIVE
# ============================================================================

cat(rep("=", 80), "\n", sep = "")
cat("BENCHMARK 3: SEASONAL NAIVE\n")
cat(rep("=", 80), "\n", sep = "")
cat("Predicts: Value from same period last cycle (e.g., 7 days ago for weekly)\n\n")

# For daily financial data, use weekly seasonality (7 days)
seasonal_period <- 7

seasonal_naive_forecast <- numeric(n_test)
for (i in 1:n_test) {
  # Look back seasonal_period days
  lookback_idx <- length(train_prices) + i - seasonal_period
  
  if (lookback_idx > 0 && lookback_idx <= length(train_prices)) {
    seasonal_naive_forecast[i] <- train_prices[lookback_idx]
  } else if (lookback_idx > length(train_prices)) {
    # Use test data if we're beyond training
    test_idx <- lookback_idx - length(train_prices)
    seasonal_naive_forecast[i] <- test_prices[test_idx]
  } else {
    # Fallback to naive if not enough history
    seasonal_naive_forecast[i] <- ifelse(i == 1, 
                                         train_prices[length(train_prices)],
                                         test_prices[i-1])
  }
}

seasonal_naive_error <- test_prices - seasonal_naive_forecast
seasonal_naive_rmse <- sqrt(mean(seasonal_naive_error^2))
seasonal_naive_mae <- mean(abs(seasonal_naive_error))
seasonal_naive_mape <- mean(abs(seasonal_naive_error / test_prices)) * 100

cat(sprintf("  Using %d-day seasonality\n", seasonal_period))
cat(sprintf("  RMSE: $%.4f\n", seasonal_naive_rmse))
cat(sprintf("  MAE:  $%.4f\n", seasonal_naive_mae))
cat(sprintf("  MAPE: %.2f%%\n\n", seasonal_naive_mape))

# ============================================================================
# 6. BENCHMARK MODEL 4: DRIFT METHOD
# ============================================================================

cat(rep("=", 80), "\n", sep = "")
cat("BENCHMARK 4: DRIFT METHOD\n")
cat(rep("=", 80), "\n", sep = "")
cat("Predicts: Linear trend from first to last training value\n\n")

# Calculate drift (average change per period)
drift <- (train_prices[length(train_prices)] - train_prices[1]) / (length(train_prices) - 1)

drift_forecast <- numeric(n_test)
for (i in 1:n_test) {
  drift_forecast[i] <- train_prices[length(train_prices)] + drift * i
}

drift_error <- test_prices - drift_forecast
drift_rmse <- sqrt(mean(drift_error^2))
drift_mae <- mean(abs(drift_error))
drift_mape <- mean(abs(drift_error / test_prices)) * 100

cat(sprintf("  Drift: $%.6f per day\n", drift))
cat(sprintf("  RMSE: $%.4f\n", drift_rmse))
cat(sprintf("  MAE:  $%.4f\n", drift_mae))
cat(sprintf("  MAPE: %.2f%%\n\n", drift_mape))

# ============================================================================
# 7. BENCHMARK MODEL 5: AR(1) - Autoregressive Order 1
# ============================================================================

cat(rep("=", 80), "\n", sep = "")
cat("BENCHMARK 5: AR(1) MODEL\n")
cat(rep("=", 80), "\n", sep = "")
cat("Predicts: Using autoregressive model of order 1\n\n")

# Fit AR(1) model on training data
ar1_model <- tryCatch({
  Arima(train_prices, order = c(1, 0, 0), include.mean = TRUE)
}, error = function(e) {
  cat("  AR(1) model failed, using fallback...\n")
  NULL
})

if (!is.null(ar1_model)) {
  # Forecast for test period
  ar1_forecast_obj <- forecast(ar1_model, h = n_test)
  ar1_forecast <- as.numeric(ar1_forecast_obj$mean)
  
  ar1_error <- test_prices - ar1_forecast
  ar1_rmse <- sqrt(mean(ar1_error^2))
  ar1_mae <- mean(abs(ar1_error))
  ar1_mape <- mean(abs(ar1_error / test_prices)) * 100
  
  cat(sprintf("  AR(1) coefficient: %.4f\n", coef(ar1_model)["ar1"]))
  cat(sprintf("  RMSE: $%.4f\n", ar1_rmse))
  cat(sprintf("  MAE:  $%.4f\n", ar1_mae))
  cat(sprintf("  MAPE: %.2f%%\n\n", ar1_mape))
} else {
  ar1_forecast <- rep(NA, n_test)
  ar1_rmse <- NA
  ar1_mae <- NA
  ar1_mape <- NA
  cat("  AR(1) model could not be fitted\n\n")
}

# ============================================================================
# 8. GP MODEL 6: SINGLE GP MODEL (Using uso_simple_robust methodology)
# ============================================================================

cat(rep("=", 80), "\n", sep = "")
cat("MODEL 6: GAUSSIAN PROCESS (SINGLE MODEL - uso_simple_robust)\n")
cat(rep("=", 80), "\n", sep = "")
cat("Training GP model with grid search optimization...\n\n")

# Create features EXACTLY as in uso_simple_robust.R
create_features <- function(data, n_lags = 5) {
  features <- data
  
  # Create 5 lags (EXACT match to uso_simple_robust)
  for (lag in 1:n_lags) {
    features[[paste0("lag_", lag)]] <- dplyr::lag(data$price, lag)
  }
  
  # Simple moving averages (EXACT match)
  features$ma_5 <- rollmean(data$price, 5, fill = NA, align = "right")
  features$ma_10 <- rollmean(data$price, 10, fill = NA, align = "right")
  
  # Remove NA rows
  features <- features %>% filter(complete.cases(.))
  
  return(features)
}

uso_features <- create_features(uso_data, n_lags = 5)

# Re-split with features (EXACT same split as uso_simple_robust)
n_total_feat <- nrow(uso_features)
n_test_feat <- round(n_total_feat * 0.2)
split_index_feat <- n_total_feat - n_test_feat

train_data_feat <- uso_features[1:split_index_feat, ]
test_data_feat <- uso_features[(split_index_feat + 1):n_total_feat, ]

feature_cols <- names(uso_features)[!names(uso_features) %in% c("date", "price")]
X_train <- as.matrix(train_data_feat[, feature_cols])
y_train <- train_data_feat$price
X_test <- as.matrix(test_data_feat[, feature_cols])
y_test <- test_data_feat$price

cat(sprintf("Features: %d (5 lags + 2 moving averages)\n", length(feature_cols)))
cat(sprintf("Train: %d observations | Test: %d observations\n\n", 
            nrow(train_data_feat), nrow(test_data_feat)))

# Standardize (EXACT match to uso_simple_robust)
train_mean <- colMeans(X_train)
train_sd <- apply(X_train, 2, sd)
train_sd[train_sd == 0] <- 1
X_train_scaled <- scale(X_train, center = train_mean, scale = train_sd)
X_test_scaled <- scale(X_test, center = train_mean, scale = train_sd)

# Grid search with EXACT same parameters as uso_simple_robust
cat("Grid search for optimal hyperparameters...\n")
param_grid <- expand.grid(
  sigma = c(0.05, 0.1, 0.2, 0.5),
  variance = c(0.01, 0.05, 0.1)
)

cat(sprintf("Testing %d parameter combinations with 3-fold CV...\n\n", 
            nrow(param_grid)))

# Cross-validation function (EXACT match to uso_simple_robust)
evaluate_params <- function(sigma, variance, X, y) {
  
  k_folds <- 3  # Match uso_simple_robust
  n <- nrow(X)
  fold_size <- floor(n / k_folds)
  rmse_folds <- numeric(k_folds)
  
  for (fold in 1:k_folds) {
    test_start <- (fold - 1) * fold_size + 1
    test_end <- min(fold * fold_size, n)
    test_idx <- test_start:test_end
    train_idx <- setdiff(1:n, test_idx)
    
    tryCatch({
      model <- gausspr(
        X[train_idx, ], y[train_idx],
        kernel = "rbfdot",
        kpar = list(sigma = sigma),
        var = variance,
        type = "regression",
        tol = 0.01  # Match uso_simple_robust
      )
      
      pred <- predict(model, X[test_idx, ])
      rmse_folds[fold] <- sqrt(mean((pred - y[test_idx])^2))
      
    }, error = function(e) {
      rmse_folds[fold] <<- NA
    })
  }
  
  valid_rmse <- rmse_folds[!is.na(rmse_folds)]
  if (length(valid_rmse) == 0) return(NA)
  
  return(mean(valid_rmse))
}

# Evaluate all combinations
results_list <- list()
for (i in 1:nrow(param_grid)) {
  sigma <- param_grid$sigma[i]
  variance <- param_grid$variance[i]
  
  cv_rmse <- evaluate_params(sigma, variance, X_train_scaled, y_train)
  
  results_list[[i]] <- data.frame(
    sigma = sigma,
    variance = variance,
    cv_rmse = cv_rmse
  )
  
  cat(sprintf("  σ=%.2f, var=%.2f -> CV RMSE: %.4f\n", 
              sigma, variance, cv_rmse))
}

results_df <- bind_rows(results_list) %>% filter(!is.na(cv_rmse))

# Get best parameters
best_idx <- which.min(results_df$cv_rmse)
best_sigma <- results_df$sigma[best_idx]
best_variance <- results_df$variance[best_idx]

cat("\n")
cat(sprintf("Optimal: σ=%.4f, var=%.4f (CV RMSE: %.4f)\n\n", 
            best_sigma, best_variance, results_df$cv_rmse[best_idx]))

# Train final GP single model (EXACT match to uso_simple_robust)
cat("Training final GP single model...\n")
gp_single_model <- gausspr(
  X_train_scaled, y_train,
  kernel = "rbfdot",
  kpar = list(sigma = best_sigma),
  var = best_variance,
  type = "regression"
)

gp_single_forecast <- as.numeric(predict(gp_single_model, X_test_scaled))

gp_single_error <- y_test - gp_single_forecast
gp_single_rmse <- sqrt(mean(gp_single_error^2))
gp_single_mae <- mean(abs(gp_single_error))
gp_single_mape <- mean(abs(gp_single_error / y_test)) * 100

cat(sprintf("  RMSE: $%.4f\n", gp_single_rmse))
cat(sprintf("  MAE:  $%.4f\n", gp_single_mae))
cat(sprintf("  MAPE: %.2f%%\n\n", gp_single_mape))

# ============================================================================
# 9. GP MODEL 7: ENSEMBLE OF 10 GP MODELS (Using uso_ensemble_gpr methodology)
# ============================================================================

cat(rep("=", 80), "\n", sep = "")
cat("MODEL 7: GAUSSIAN PROCESS ENSEMBLE (10 MODELS - uso_ensemble_gpr)\n")
cat(rep("=", 80), "\n", sep = "")
cat("Training ensemble of 10 GP models with bootstrap sampling...\n\n")

n_models <- 10
all_predictions <- matrix(NA, nrow = length(y_test), ncol = n_models)

for (i in 1:n_models) {
  cat(sprintf("Training model %d/%d...\n", i, n_models))
  
  # Bootstrap sample (EXACT match to uso_ensemble_gpr)
  boot_indices <- sample(1:nrow(X_train_scaled), 
                        size = nrow(X_train_scaled), 
                        replace = TRUE)
  
  X_boot <- X_train_scaled[boot_indices, ]
  y_boot <- y_train[boot_indices]
  
  tryCatch({
    model <- gausspr(X_boot, y_boot,
                    kernel = "rbfdot",
                    kpar = list(sigma = best_sigma),
                    var = best_variance,
                    type = "regression")
    
    pred <- predict(model, X_test_scaled)
    all_predictions[, i] <- as.vector(pred)
    
    cat(sprintf("  Model %d trained successfully\n", i))
    
  }, error = function(e) {
    cat(sprintf("  Model %d FAILED: %s\n", i, e$message))
    all_predictions[, i] <<- NA
  })
}

cat("\n")

# Average predictions (EXACT match to uso_ensemble_gpr)
gp_ensemble_forecast <- rowMeans(all_predictions, na.rm = TRUE)
pred_std <- apply(all_predictions, 1, sd, na.rm = TRUE)

gp_ensemble_error <- y_test - gp_ensemble_forecast
gp_ensemble_rmse <- sqrt(mean(gp_ensemble_error^2))
gp_ensemble_mae <- mean(abs(gp_ensemble_error))
gp_ensemble_mape <- mean(abs(gp_ensemble_error / y_test)) * 100

valid_models <- sum(colSums(!is.na(all_predictions)) > 0)
cat(sprintf("Successfully trained %d/%d models\n", valid_models, n_models))
cat(sprintf("Average prediction uncertainty: $%.4f\n\n", mean(pred_std)))
cat(sprintf("  RMSE: $%.4f\n", gp_ensemble_rmse))
cat(sprintf("  MAE:  $%.4f\n", gp_ensemble_mae))
cat(sprintf("  MAPE: %.2f%%\n\n", gp_ensemble_mape))
```

# ============================================================================

# 10. COMPILE RESULTS AND COMPARISON

# ============================================================================

```{r}
cat("\n", rep("=", 80), "\n", sep = "")
cat("COMPREHENSIVE BENCHMARK COMPARISON\n")
cat(rep("=", 80), "\n\n", sep = "")

# Create results table
benchmark_results <- data.frame(
  Model = c("1. Mean", "2. Naive", "3. Seasonal Naive", "4. Drift", 
            "5. AR(1)", "6. GP Single", "7. GP Ensemble"),
  RMSE = c(mean_rmse, naive_rmse, seasonal_naive_rmse, drift_rmse, 
           ar1_rmse, gp_single_rmse, gp_ensemble_rmse),
  MAE = c(mean_mae, naive_mae, seasonal_naive_mae, drift_mae, 
          ar1_mae, gp_single_mae, gp_ensemble_mae),
  MAPE = c(mean_mape, naive_mape, seasonal_naive_mape, drift_mape, 
           ar1_mape, gp_single_mape, gp_ensemble_mape)
)

# Calculate relative metrics (compared to naive baseline)
benchmark_results$RRMSE <- (benchmark_results$RMSE / mean(test_prices)) * 100
benchmark_results$RMSE_vs_Naive <- (benchmark_results$RMSE / naive_rmse) * 100
benchmark_results$MAE_vs_Naive <- (benchmark_results$MAE / naive_mae) * 100

# Print formatted table
cat("Performance Metrics:\n")
cat(rep("-", 80), "\n", sep = "")
cat(sprintf("%-20s %10s %10s %10s %10s\n", 
            "Model", "RMSE", "MAE", "MAPE", "RRMSE"))
cat(rep("-", 80), "\n", sep = "")

for (i in 1:nrow(benchmark_results)) {
  cat(sprintf("%-20s $%9.4f $%9.4f %9.2f%% %9.2f%%\n",
              benchmark_results$Model[i],
              benchmark_results$RMSE[i],
              benchmark_results$MAE[i],
              benchmark_results$MAPE[i],
              benchmark_results$RRMSE[i]))
}

cat(rep("-", 80), "\n\n", sep = "")

# Rankings
cat("Model Rankings (Lower is Better):\n")
cat(rep("-", 80), "\n", sep = "")

rmse_rank <- rank(benchmark_results$RMSE)
mae_rank <- rank(benchmark_results$MAE)
mape_rank <- rank(benchmark_results$MAPE)

cat(sprintf("%-20s %10s %10s %10s\n", "Model", "RMSE Rank", "MAE Rank", "MAPE Rank"))
cat(rep("-", 80), "\n", sep = "")

for (i in 1:nrow(benchmark_results)) {
  cat(sprintf("%-20s %10d %10d %10d\n",
              benchmark_results$Model[i],
              rmse_rank[i],
              mae_rank[i],
              mape_rank[i]))
}

cat(rep("-", 80), "\n\n", sep = "")

# Best model
best_model_idx <- which.min(benchmark_results$RMSE)
cat(sprintf("BEST MODEL (by RMSE): %s\n", benchmark_results$Model[best_model_idx]))
cat(sprintf("  RMSE: $%.4f\n", benchmark_results$RMSE[best_model_idx]))
cat(sprintf("  Improvement over Naive: %.2f%%\n", 
            100 - benchmark_results$RMSE_vs_Naive[best_model_idx]))
```

# ============================================================================

# 11. SAVE RESULTS

# ============================================================================

```{r}
cat("\n", rep("=", 80), "\n", sep = "")
cat("SAVING RESULTS\n")
cat(rep("=", 80), "\n\n", sep = "")

# Save benchmark comparison table
benchmark_file <- file.path(output_dir, "benchmark_comparison.csv")
write.csv(benchmark_results, benchmark_file, row.names = FALSE)
cat(sprintf("Saved: %s\n", benchmark_file))

# Detailed predictions (align all to same length - use GP test set)
predictions_df <- data.frame(
  date = test_data_feat$date,
  actual = y_test,
  gp_single_pred = gp_single_forecast,
  gp_ensemble_pred = gp_ensemble_forecast
)

# For benchmark models, we need to align them to the GP test dates
# Since benchmarks used raw data and GP used features, dates might differ
# We'll save them separately

# Save benchmark predictions (from raw data split)
benchmark_preds <- data.frame(
  date = test_dates[1:min(length(test_dates), length(mean_forecast))],
  actual = test_prices[1:min(length(test_prices), length(mean_forecast))],
  mean_pred = mean_forecast[1:min(length(mean_forecast), length(test_prices))],
  naive_pred = naive_forecast[1:min(length(naive_forecast), length(test_prices))],
  seasonal_naive_pred = seasonal_naive_forecast[1:min(length(seasonal_naive_forecast), length(test_prices))],
  drift_pred = drift_forecast[1:min(length(drift_forecast), length(test_prices))],
  ar1_pred = ar1_forecast[1:min(length(ar1_forecast), length(test_prices))]
)

benchmark_preds_file <- file.path(output_dir, "benchmark_predictions.csv")
write.csv(benchmark_preds, benchmark_preds_file, row.names = FALSE)
cat(sprintf("Saved: %s\n", benchmark_preds_file))

# Save GP model predictions
gp_preds_file <- file.path(output_dir, "gp_model_predictions.csv")
write.csv(predictions_df, gp_preds_file, row.names = FALSE)
cat(sprintf("Saved: %s\n", gp_preds_file))

# Save grid search results
grid_results_file <- file.path(output_dir, "grid_search_results.csv")
write.csv(results_df, grid_results_file, row.names = FALSE)
cat(sprintf("Saved: %s\n", grid_results_file))

# Save individual ensemble model predictions
ensemble_all_preds <- as.data.frame(all_predictions)
colnames(ensemble_all_preds) <- paste0("Model_", 1:n_models)
ensemble_all_preds$date <- test_data_feat$date
ensemble_all_preds$actual <- y_test
ensemble_all_preds$ensemble_avg <- gp_ensemble_forecast

ensemble_preds_file <- file.path(output_dir, "ensemble_individual_predictions.csv")
write.csv(ensemble_all_preds, ensemble_preds_file, row.names = FALSE)
cat(sprintf("Saved: %s\n", ensemble_preds_file))

# Save summary statistics
summary_stats <- data.frame(
  Metric = c("n_train_raw", "n_test_raw", "n_train_features", "n_test_features",
             "best_sigma", "best_variance", "n_ensemble_models", "n_valid_ensemble"),
  Value = c(length(train_prices), length(test_prices), 
            nrow(train_data_feat), nrow(test_data_feat),
            best_sigma, best_variance, n_models, valid_models)
)

summary_file <- file.path(output_dir, "benchmark_summary_stats.csv")
write.csv(summary_stats, summary_file, row.names = FALSE)
cat(sprintf("Saved: %s\n", summary_file))

cat("\n")
cat("All results saved successfully to:\n")
cat(sprintf("  %s\n\n", output_dir))

cat(rep("=", 80), "\n", sep = "")
cat("BENCHMARKING COMPLETE!\n")
cat(rep("=", 80), "\n\n", sep = "")

cat("Files created:\n")
cat("  1. benchmark_comparison.csv - Main results table\n")
cat("  2. benchmark_predictions.csv - Predictions from simple models\n")
cat("  3. gp_model_predictions.csv - Predictions from GP models\n")
cat("  4. grid_search_results.csv - Hyperparameter search results\n")
cat("  5. ensemble_individual_predictions.csv - All 10 ensemble model predictions\n")
cat("  6. benchmark_summary_stats.csv - Summary statistics\n")
cat("\n")

cat("Summary:\n")
cat(sprintf("  Best model: %s\n", benchmark_results$Model[best_model_idx]))
cat(sprintf("  Best RMSE: $%.4f (RRMSE: %.2f%%)\n", 
            benchmark_results$RMSE[best_model_idx],
            benchmark_results$RRMSE[best_model_idx]))
cat(sprintf("  Improvement over Naive: %.2f%%\n\n", 
            100 - benchmark_results$RMSE_vs_Naive[best_model_idx]))
```
