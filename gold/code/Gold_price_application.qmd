---
title: "Gold_price_application"
format: html
---

-ARIMA/AR(1) assumes a fixed functional form (linear dependence on past lags)

-Gaussian Process (GP) regression does not assume a fixed equation.
-> Instead of saying "gold price depends linearly on yesterday’s price," GP says "gold price is a random function correlated across time (and potentially across covariates like oil, stock indices)

Why GPs might help for gold prices
-Gold depends on multiple noisy drivers (oil, stocks, currencies).
-GP can flexibly model nonlinear functional relationships between these predictors and gold price without you specifying the exact formula.

test: Do these external variables help explain/predict gold prices better than just time-based ARIMA?

##Close vs adj close?
-If your goal is forecasting actual market prices (e.g., “what will the ETF close at tomorrow?”), use Close.

-If your goal is analyzing underlying economic drivers or comparing with other assets (e.g., oil, silver, indices), use Adj Close because it gives a consistent price series.

-If you use Adj Close, you might “smooth out” some changes that actually happened in the market.

##Rule of Thumb:

For modeling price movements/trading strategies → use Close.

For modeling long-term value/returns → use Adj Close.




##1. EDA 

1.1 We’ll first read in the dataset, check dimensions, missing values, and focus on the Close price of gold.
```{r}
library(tidyverse)
library(lubridate)

# Load dataset
gold <- read.csv("FINAL_USO.csv")

# Inspect structure
str(gold)
summary(gold)

# Convert Date to proper date type
gold$Date <- as.Date(gold$Date, format="%Y-%m-%d")

# Sort by Date just in case
gold <- gold %>% arrange(Date)

# Focus on Gold's Close
gold_close <- gold %>%
  select(Date, Close) %>%
  rename(Gold_Close = Close)

head(gold_close)

# Inspect structure
str(gold)

# Summary statistics
summary(gold)

# Check for missing values
colSums(is.na(gold))
```
1.2) Time-series plot to see overall trends & volatility.

```{r}
ggplot(gold_close, aes(Date, Gold_Close)) +
  geom_line(color="goldenrod") +
  theme_minimal() +
  labs(title="Gold Closing Price (USD)",
       x="Date", y="Close Price (USD)")

```
=> mean level changes a bit-> gp picks this up -> not with squared expo-> process wih sudden jumps at small distances, variance constant around jumps-. model mean and var around that-> 2 var parameters in GP

=> capture jumps -> pat of var -> can do this in future 

This will show if gold is trending, mean-reverting, or highly volatile.

Observations from the plot
Trend: Gold prices fell sharply from late 2012–2013, then entered a long sideways/oscillating movement until 2019.

Volatility: Periods of high volatility (2011–2013) followed by calmer fluctuations (2016–2018).

No obvious seasonality: At first glance, there’s no clear seasonal repeating pattern like in monthly sales or temperature data.

Shumway & Stoffer put decomposition under exploratory tools — it helps see seasonality or trend before deciding on ARIMA differencing or GP kernel design.
```{r}
library(astsa)

# Convert Closing Price to a time series object
# daily → assume ~252 trading days per year
ts_gold <- ts(gold$Close, frequency = 1, start = c(2011, 11))

#multiplicative-> season and residuals are multiplicative with trend 
library(forecast)
fit_ets <- ets(ts_gold)
summary(fit_ets)
plot(fit_ets)

```


```{r}
library(forecast)
loess_trend <- ma(ts_gold, order = 30)  # Simple moving average
plot(ts_gold, main = "Gold Price with Smoothed Trend")
lines(loess_trend, col = "blue", lwd = 2)

```


```{r}
fit_ets <- ets(ts_gold)
forecast_ets <- forecast(fit_ets, h=100)
plot(forecast_ets)


```



```{r}
library(forecast)

# Training window: last 500 obs
ts_train <- window(ts_gold, start = length(ts_gold) - 500)

fit_naive <- naive(ts_train, h=30)
fit_arima <- auto.arima(ts_train)
fit_garch <- auto.arima(ts_train) # replace residuals with GARCH later

autoplot(forecast(fit_naive, h=30)) + ggtitle("Naive Forecast")
autoplot(forecast(fit_arima, h=30)) + ggtitle("ARIMA Forecast")


```


STL Decomposition of Gold Prices => check this -> seasonality in residuals

Observed Data (Top Panel)

The gold closing prices show clear long-term swings with periods of upward and downward trends.

Around 2011–2013, prices declined sharply, and after 2015 the series shows mild recovery.

Volatility (spread of fluctuations) seems to change over time, suggesting some heteroskedasticity.

Seasonal Component (Second Panel)

There are short-term repeating oscillations (likely annual/quarterly effects), but their magnitude is quite small compared to overall price level changes.

This suggests weak but present seasonality, which may not dominate the dynamics.

For ARIMA, we might or might not include seasonal differencing. For GP, this could motivate testing a periodic kernel, though the effect is modest.

##Trend Component (Third Panel)

Clear downward trend from 2011 through 2013.

After 2014, the trend stabilizes with mild upward drift.

This confirms the ADF result earlier (non-stationary).

In ARIMA, we’d need differencing (d=1).

In GP, this motivates including a Linear kernel to model the drift directly.

##Remainder (Bottom Panel)

Captures short-term unpredictable movements (residuals).

Shows bursts of volatility around 2012 and again around 2016–2017.

This reminds us of financial data characteristics: shocks, volatility clustering

```{r}
library(tseries)
library(forecast)

# 1. KPSS test (null = stationary)
kpss_test <- kpss.test(ts_gold, null = "Level")
print(kpss_test)

# 2. Periodogram for seasonality
spec.pgram(ts_gold, log="no", main="Periodogram of Gold Closing Price")

# 3. Seasonal strength (from STL decomposition we already did)
stl_fit <- stl(ts_gold, s.window="periodic")
summary(stl_fit$time.series)

# 4. Rolling variance to check volatility clustering
roll_var <- zoo::rollapply(ts_gold, width=50, FUN=var, align="right", fill=NA)
plot(roll_var, type="l", main="Rolling Variance (Window = 365 days)", ylab="Variance", xlab="Time")

```
##Stationarity Testing (KPSS)
KPSS Test result: 

KPSS=10.24,p<0.01.
means we Reject the null of stationarity.

The series is non-stationary at levels → differencing or detrending will be needed for ARIMA.

For GP, we can directly encode this via kernels (e.g., linear trend kernel).

##Periodogram
The periodogram shows strong low-frequency power.

most of the variability is explained by slow-moving components (trend) rather than sharp seasonal cycles.

No sharp spikes at higher frequencies → weak fixed periodicity.


##Rolling Variance:

The rolling variance plot showed volatility clustering (large fluctuations in certain periods, e.g. around 2012–2013).

This confirms heteroskedasticity — variance is not constant over time.

note:time-varying volatility, i.e. heteroskedasticity.
This is different from variance being proportional to the mean level (which logging fixes).

volatility clustering → calm and stormy periods alternate.
So, even if your ARIMA captures the mean dynamics, it will fail on the variance structure.




We used a 50-day rolling window to compute variance, consistent with standard practice in financial econometrics (Tsay, 2010). This window length strikes a balance between capturing short-term volatility clustering (Engle, 1982; Bollerslev, 1986) and avoiding excessive noise. The resulting variance plot reveals pronounced heteroskedasticity, with clusters of high and low volatility over time.”

1.3) Since ARIMA models need stationarity, we’ll check with ACF/PACF and ADF test.
```{r}
library(tseries)

ts_gold <- ts(gold$Close, frequency = 252)  # approx 252 trading days per year

# Plot ACF and PACF
par(mfrow=c(1,2))
acf(ts_gold, main="ACF of Gold Closing Price")
pacf(ts_gold, main="PACF of Gold Closing Price")


# Augmented Dickey-Fuller test
adf.test(ts_gold)

```
If p-value > 0.05 → non-stationary, need differencing (ARIMA with d>0).

If p-value < 0.05 → stationary.


ACF: The autocorrelation function shows extremely high correlation across many lags, gradually decaying. This is a strong sign of non-stationarity. In a stationary series, the ACF should cut off relatively quickly.

PACF: The partial autocorrelation shows one big spike at lag 1, then quickly flattens. This is also typical of a non-stationary series (like a random walk).

Since the p-value >> 0.05, we fail to reject the null hypothesis of a unit root. This confirms what the ACF/PACF suggest: the gold closing price series is non-stationary



##Constrution of kernel 
Time-Series Workflow (Time Series Analysis and Its Applications (Shumway & Stoffer) and Brockwell & Davis.)
1. Exploratory Data Analysis (EDA)

Plot the series: check for trend, volatility, seasonality.

Sample ACF/PACF: helps see persistence, autocorrelation strength.

Unit root tests (ADF, KPSS): check stationarity.

-In ARIMA, this dictates whether we need differencing.
- For GP, this guides kernel design (trend → linear, short memory → Matérn, cycles → periodic).

2. Transformations

Log transform: stabilizes variance if needed.

Differencing: used for ARIMA to remove trend/seasonality.

ARIMA requires stationary data.

GP does not require differencing → we keep levels and encode trend directly.

3. Model Identification

ARIMA side (Box-Jenkins):

-Inspect ACF/PACF → choose p, q, d.

-Seasonal differencing if seasonality detected.

GP side:

Decide kernel structure based on EDA:

-If trending → add Linear kernel.

-If short memory → add Matérn/SE.

-If seasonality → add Periodic.

-Always include Noise.

4. Parameter Estimation

-ARIMA: maximum likelihood (via Arima() in R).

-GP: hyperparameter estimation via marginal likelihood (Stan).


5. Diagnostic Checking

-ARIMA: residuals → check if white noise.

-GP: posterior predictive checks, compare actual vs predictive mean ± CI.

 6. Forecasting & Comparison

ARIMA: forecast h-steps ahead.

GP: posterior predictive distribution at future points.

Compare using RMSE, MAE, interval coverage on a test set.

##step 1 (might not be necessary)
```{r}
library(ggplot2)
library(forecast)

# Convert date column
gold$Date <- as.Date(gold$Date)

# Close price
ggplot(gold, aes(x = Date, y = Close)) +
  geom_line(color = "goldenrod") +
  labs(title = "Gold Price (Close)", x = "Date", y = "Close Price") +
  theme_minimal()

# log-transformed Close
gold$logClose <- log(gold$Close)

ggplot(gold, aes(x = Date, y = logClose)) +
  geom_line(color = "darkblue") +
  labs(title = "Log-Transformed Gold Price", x = "Date", y = "Log(Close)") +
  theme_minimal()

# Compute log returns
gold$logReturn <- c(NA, diff(log(gold$Close)))

ggplot(gold, aes(x = Date, y = logReturn)) +
  geom_line(color = "purple") +
  labs(title = "Gold Log Returns", x = "Date", y = "Log Return") +
  theme_minimal()



```

note: magnitude of fluctuations does not clearly grow with the mean level. So visually, logging doesn’t change much.


Log transformations: stationarizes the data better by removing both trend and scale effects.

The third plot (log differences) looks stationary — no clear trend, mean-reverting fluctuations, and a relatively stable variance (aside from volatility bursts, typical of financial data).

This transformation achieves stationarity, which is essential for ARIMA.



 use ARIMA + GARCH?

-ARIMA handles trend & autocorrelation in the mean.

-GARCH handles time-varying volatility in the variance.



Fit Guassian process: Method 1 (Latent variable method)

```{r}
gold <- read.csv("FINAL_USO.csv")
gold$Date <- as.Date(gold$Date, format="%Y-%m-%d")

head(gold)

```


```{r}
library(cmdstanr)
# Use first 1500 obs for training, next 100 for forecasting
Ntrain <- 1500
x1 <- 1:300
y1 <- scale(gold$Close[1:300], center = TRUE, scale = TRUE)
# Prediction horizon: next 20 days
x2 <- 301:320

stan_data <- list(
  N1 = length(x1),
  x1 = as.array(x1),
  y1 = as.vector(y1),
  N2 = length(x2),
  x2 = as.array(x2)
)

mod <- cmdstan_model("gold.stan")
fit <- mod$sample(data = stan_data, chains = 1, iter_sampling = 2000) #draws samples from the posterior distribution of parameters given data 
```


```{r}
# data frame of posterior draws
posterior_df <- fit$draws(c("rho", "alpha", "sigma")) %>%
  posterior::as_draws_df()

head(posterior_df)

```
```{r}
# Posterior summaries (mean, sd, quantiles)
posterior_summary <- posterior::summarise_draws(posterior_df)
print(posterior_summary)

```

```{r}
library(cmdstanr)
library(posterior)
library(ggplot2)
#Extract posterior draws of f
posterior <- fit$draws("f")   # extract only latent function draws
f_array <- posterior %>% posterior::as_draws_matrix()

# Summarize across posterior draws
f_mean  <- apply(f_array, 2, mean)
f_lower <- apply(f_array, 2, quantile, probs = 0.025)
f_upper <- apply(f_array, 2, quantile, probs = 0.975)



# Suppose your fitted object is called `fit`
# Extract all draws for f
f_draws <- fit$draws("f")

# Convert to matrix (iterations × N)
f_matrix <- as_draws_matrix(f_draws)


dim(f_matrix)   #cols = N1 + N2

# Pick, say, 20 random posterior draws
set.seed(123)
idx <- sample(1:nrow(f_matrix), 20)

# Get subset for plotting
f_samples <- f_matrix[idx, ]

# Convert to tidy format for ggplot
N <- ncol(f_matrix)
f_df <- data.frame(
  x = c(x1, x2),   # observed + forecast inputs
  t(f_samples)     # transpose draws to align with x
)

# Reshape long
f_long <- reshape2::melt(f_df, id.vars="x")

ggplot(f_long, aes(x=x, y=value, group=variable)) +
  geom_line(alpha=0.5, color="brown") +
  geom_point(data=data.frame(x=x1, y=y1), aes(x, y), color="black") +
  theme_minimal() +
  labs(title="Posterior GP Sample Paths (Latent Function)",
       x="Time", y="f(x)") 

```

```{r}
# Extract all draws for f
f_draws <- fit$draws("f")

# Convert to matrix (iterations × N)
f_matrix <- as_draws_matrix(f_draws)

# Posterior mean and 95% CI for f
f_mean <- apply(f_matrix, 2, mean)
f_lower <- apply(f_matrix, 2, quantile, 0.025)
f_upper <- apply(f_matrix, 2, quantile, 0.975)

df_summary <- data.frame(
  x = c(x1, x2),
  mean = f_mean,
  lower = f_lower,
  upper = f_upper
)

# Plot posterior mean + CI
ggplot(df_summary, aes(x=x, y=mean)) +
  geom_ribbon(aes(ymin=lower, ymax=upper), fill="lightblue", alpha=0.4) +
  geom_line(color="blue", size=1) +
  geom_point(data=data.frame(x=x1, y=y1), aes(x, y), inherit.aes=FALSE, color="black", size=1) +
  theme_minimal() +
  labs(title="GP Posterior Mean with 95% Credible Interval",
       x="Time", y="f(x)")

```
```{r}
# Extract posterior summaries for latent f
gp_summary <- fit$summary(variables = c("f"))

# Rescale back to original data scale
f_mean  <- gp_summary$mean
f_sd    <- gp_summary$sd

f_rescaled <- f_mean * y_sd + y_mean
f_lower    <- (f_mean - 1.96 * f_sd) * y_sd + y_mean
f_upper    <- (f_mean + 1.96 * f_sd) * y_sd + y_mean

# Dimensions
N1 <- length(x1)
N2 <- length(x2)
N  <- N1 + N2

# Training (observed)
df_train <- data.frame(
  x = x1,
  f_mean = f_rescaled[1:N1],
  lower  = f_lower[1:N1],
  upper  = f_upper[1:N1]
)

# Prediction (future)
df_pred <- data.frame(
  x = x2,
  f_mean = f_rescaled[(N1+1):N],
  lower  = f_lower[(N1+1):N],
  upper  = f_upper[(N1+1):N]
)

```

```{r}
library(ggplot2)

# Extract posterior summaries for latent f
gp_summary <- fit$summary(variables = c("f"))

# Rescale back to original data scale
f_mean  <- gp_summary$mean
f_sd    <- gp_summary$sd

f_rescaled <- f_mean * y_sd + y_mean
f_lower    <- (f_mean - 1.96 * f_sd) * y_sd + y_mean
f_upper    <- (f_mean + 1.96 * f_sd) * y_sd + y_mean

# Dimensions
N1 <- length(x1)
N2 <- length(x2)
N  <- N1 + N2

# Training
df_train <- data.frame(
  x = x1,
  f_mean = f_rescaled[1:N1],
  lower  = f_lower[1:N1],
  upper  = f_upper[1:N1],
  type = "Train"
)

# Prediction 
df_pred <- data.frame(
  x = x2,
  f_mean = f_rescaled[(N1+1):N],
  lower  = f_lower[(N1+1):N],
  upper  = f_upper[(N1+1):N],
  type = "Forecast"
)


df_plot <- rbind(df_train, df_pred)

# Plot
ggplot() +
  # Observed data 
  geom_point(data = data.frame(x = x1, y = as.numeric(y1) * y_sd + y_mean),
             aes(x, y), color = "black", size = 1) +
  # GP mean
  geom_line(data = df_plot, aes(x, f_mean, color = type), size = 1) +
  # GP credible interval
  geom_ribbon(data = df_plot, aes(x, ymin = lower, ymax = upper, fill = type), alpha = 0.2) +
  scale_color_manual(values = c("Train" = "blue", "Forecast" = "red")) +
  scale_fill_manual(values = c("Train" = "blue", "Forecast" = "red")) +
  theme_minimal() +
  labs(title = "Gaussian Process Fit & Forecast on Gold Prices",
       x = "Time", y = "Gold Price (USD)")

```


```{r}
library(bayesplot)

mcmc_dens(posterior_df, pars = c("rho", "alpha", "sigma"))
mcmc_trace(posterior_df, pars = c("rho", "alpha", "sigma"))

```
 the gold price (after scaling) varies smoothly over ~4 days (ρ ≈ 4), with moderate signal variance (α ≈ 0.9) and relatively low observation noise (σ ≈ 0.18).

The well-behaved trace plots confirm the posterior estimates are stable.



```{r}
library(forecast)

# --- Fit ARIMA on training set only ---
fit_arima <- auto.arima(as.numeric(y1))   # use scaled training data
summary(fit_arima)

# Forecast h steps ahead
arima_forecast <- forecast(fit_arima, h = N2)

# --- Build ARIMA forecast df ---
df_arima_train <- data.frame(
  x = x1,
  mean  = fitted(fit_arima) * y_sd + y_mean
)

df_arima_forecast <- data.frame(
  x = x2,
  mean  = as.numeric(arima_forecast$mean) * y_sd + y_mean,
  lower = as.numeric(arima_forecast$lower[,2]) * y_sd + y_mean,  # 95% CI
  upper = as.numeric(arima_forecast$upper[,2]) * y_sd + y_mean
)

# --- Plot ---
ggplot() +
  # Observed data
  geom_point(data = data.frame(x = x1, y = as.numeric(y1) * y_sd + y_mean),
             aes(x, y), color = "black", size = 1) +
  
  # ARIMA training fit
  geom_line(data = df_arima_train, aes(x, mean), color = "darkgreen", size = 1) +
  
  # ARIMA forecast
  geom_line(data = df_arima_forecast, aes(x, mean), color = "green", size = 1) +
  geom_ribbon(data = df_arima_forecast, 
              aes(x, ymin = lower, ymax = upper), 
              fill = "green", alpha = 0.2) +
  
  theme_minimal() +
  labs(title = "ARIMA Training Fit & Forecast on Gold Prices",
       x = "Time", y = "Gold Price (USD)")



```
```{r}
library(forecast)

# ARIMA to training data (first N1 points)
fit_arima <- auto.arima(as.numeric(y1))   # gold$Close[1:N1] if not scaled
summary(fit_arima)


```
Fit different models and forecast

```{r}
library(forecast)
library(ggplot2)

# --- ARIMA ---
fit_arima <- auto.arima(ts_train)
fc_arima  <- forecast(fit_arima, h=Ntest)

df_arima_train <- data.frame(
  x = 1:Ntrain,
  f_mean = as.numeric(fitted(fit_arima)),
  lower = NA, upper = NA,
  type = "Train"
)

df_arima_pred <- data.frame(
  x = (Ntrain+1):(Ntrain+Ntest),
  f_mean = as.numeric(fc_arima$mean),
  lower  = as.numeric(fc_arima$lower[,2]),
  upper  = as.numeric(fc_arima$upper[,2]),
  type = "Forecast"
)

df_arima <- rbind(df_arima_train, df_arima_pred)

# --- LOESS trend model without season ---
fit_loess <- tslm(ts_train ~ trend)   # only trend
fc_loess  <- forecast(fit_loess, h=Ntest)

df_loess_train <- data.frame(
  x = 1:Ntrain,
  f_mean = as.numeric(fitted(fit_loess)),
  lower = NA, upper = NA,
  type = "Train"
)

df_loess_pred <- data.frame(
  x = (Ntrain+1):(Ntrain+Ntest),
  f_mean = as.numeric(fc_loess$mean),
  lower  = as.numeric(fc_loess$lower[,2]),
  upper  = as.numeric(fc_loess$upper[,2]),
  type = "Forecast"
)

df_loess <- rbind(df_loess_train, df_loess_pred)

```

```{r}
library(forecast)
library(ggplot2)
library(splines)
Ntotal <- length(ts_gold)
Ntrain <- 1500
Ntest  <-  100# Ntotal - Ntrain

ts_train <- ts(ts_gold[1:Ntrain], frequency = frequency(ts_gold))
ts_test  <- ts(ts_gold[(Ntrain+1):Ntotal], frequency = frequency(ts_gold))

# --- ARIMA ---
fit_arima <- auto.arima(ts_train)
fc_arima  <- forecast(fit_arima, h=Ntest)

df_arima_train <- data.frame(
  x = 1:Ntrain,
  f_mean = as.numeric(fitted(fit_arima)),
  lower = NA, upper = NA,
  type = "Train",
  model = "ARIMA"
)

df_arima_pred <- data.frame(
  x = (Ntrain+1):(Ntrain+Ntest),
  f_mean = as.numeric(fc_arima$mean),
  lower  = as.numeric(fc_arima$lower[,2]),
  upper  = as.numeric(fc_arima$upper[,2]),
  type = "Forecast",
  model = "ARIMA"
)

df_arima <- rbind(df_arima_train, df_arima_pred)


# --- LOESS trend only (via tslm with trend) ---
fit_loess <- tslm(ts_train ~ trend)
fc_loess  <- forecast(fit_loess, h=Ntest)

df_loess_train <- data.frame(
  x = 1:Ntrain,
  f_mean = as.numeric(fitted(fit_loess)),
  lower = NA, upper = NA,
  type = "Train",
  model = "LOESS Trend"
)

df_loess_pred <- data.frame(
  x = (Ntrain+1):(Ntrain+Ntest),
  f_mean = as.numeric(fc_loess$mean),
  lower  = as.numeric(fc_loess$lower[,2]),
  upper  = as.numeric(fc_loess$upper[,2]),
  type = "Forecast",
  model = "LOESS Trend"
)

df_loess <- rbind(df_loess_train, df_loess_pred)


# --- Spline regression on training set ---
time_idx <- 1:Ntrain
fit_spline <- lm(ts_train ~ ns(time_idx, df = 10))   # ns = natural spline with 10 df
fc_spline <- predict(fit_spline,
                     newdata = data.frame(time_idx = (Ntrain+1):(Ntrain+Ntest)),
                     se.fit = TRUE)

df_spline_train <- data.frame(
  x = 1:Ntrain,
  f_mean = as.numeric(fitted(fit_spline)),
  lower = NA, upper = NA,
  type = "Train",
  model = "Spline"
)

df_spline_pred <- data.frame(
  x = (Ntrain+1):(Ntrain+Ntest),
  f_mean = fc_spline$fit,
  lower  = fc_spline$fit - 1.96*fc_spline$se.fit,
  upper  = fc_spline$fit + 1.96*fc_spline$se.fit,
  type = "Forecast",
  model = "Spline"
)

df_spline <- rbind(df_spline_train, df_spline_pred)


# --- Naïve / Random Walk Forecast ---
fit_naive <- naive(ts_train, h=Ntest)

df_naive_train <- data.frame(
  x = 1:Ntrain,
  f_mean = as.numeric(fitted(fit_naive)),
  lower = NA, upper = NA,
  type = "Train",
  model = "Naive"
)

df_naive_pred <- data.frame(
  x = (Ntrain+1):(Ntrain+Ntest),
  f_mean = as.numeric(fit_naive$mean),
  lower  = as.numeric(fit_naive$lower[,2]),
  upper  = as.numeric(fit_naive$upper[,2]),
  type = "Forecast",
  model = "Naive"
)

df_naive <- rbind(df_naive_train, df_naive_pred)


# --- Combine all models ---
df_all <- rbind(df_arima, df_loess, df_spline, df_naive)

geom_point(
  data = data.frame(
    x = 1:(Ntrain+Ntest),
    y = as.numeric(c(ts_train, ts_test[1:Ntest]))
  ),
  aes(x, y), inherit.aes = FALSE, color = "black", size = 1
)

geom_point(
  data = data.frame(
    x = 1:(Ntrain+Ntest),
    y = as.numeric(c(ts_train, ts_test[1:Ntest]))
  ),
  aes(x, y),
  inherit.aes = FALSE, color = "black", size = 1
)

# --- Plot ---
ggplot(df_all, aes(x, f_mean, color = model, linetype = type)) +
  geom_line(size = 1) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = model),
              alpha = 0.15, color = NA) +
  geom_point(data = data.frame(x = 1:(Ntrain+Ntest),
                               y = as.numeric(c(ts_train, ts_test))),
             aes(x, y), inherit.aes = FALSE, color = "black", size = 1) +
  theme_minimal() +
  labs(title = "Gold Price Forecasts by Multiple Models",
       x = "Time", y = "Gold Price (USD)")

```






```{r}

# Total obs = 1718
Ntrain <- 1500
Ntest  <- 100   

# Training inputs/outputs
x1 <- 1:Ntrain
y1 <- scale(gold$Close[1:Ntrain], center = TRUE, scale = TRUE)


x2 <- (Ntrain+1):(Ntrain+Ntest)

stan_data <- list(
  N1 = length(x1),
  x1 = as.array(x1),
  y1 = as.vector(y1),
  N2 = length(x2),
  x2 = as.array(x2)
)

library(cmdstanr)
mod <- cmdstan_model("gold.stan")
fit <- mod$sample(data = stan_data, chains = 1, iter_sampling = 2000, seed = 123)


```

model 2 
```{r}
library(cmdstanr)
 
Ntrain <- 1500                  # training length
Ntest  <- 100                   # forecast horizon

x1 <- (1:Ntrain) / Ntrain       # scaled training inputs
y1 <- scale(gold$Close[1:Ntrain], center = TRUE, scale = TRUE)
x2 <- ((Ntrain+1):(Ntrain+Ntest)) / Ntrain   # scaled future inputs

stan_data <- list(
  N1 = length(x1),
  x1 = as.array(x1),    # array of reals
  y1 = as.vector(y1),   # vector
  N2 = length(x2),
  x2 = as.array(x2)
)

mod <- cmdstan_model("gold2.stan")
fit <- mod$sample(
  data = stan_data,
  chains = 2,
  iter_sampling = 1000
)

```
```{r}
# Extract posterior samples for f
f_draws <- fit$draws("f")         # posterior draws for latent function
f_summary <- fit$summary("f")     # mean, sd, quantiles

# Split into training and forecast parts
N1 <- length(x1)
N2 <- length(x2)

f_train <- f_summary$mean[1:N1]
f_forecast <- f_summary$mean[(N1+1):(N1+N2)]

f_train_lower <- f_summary$q5[1:N1]
f_train_upper <- f_summary$q95[1:N1]
f_forecast_lower <- f_summary$q5[(N1+1):(N1+N2)]
f_forecast_upper <- f_summary$q95[(N1+1):(N1+N2)]

```



