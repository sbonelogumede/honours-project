---
title: 'Gaussian Processes for Time Series Modelling'
author:
  - 'Azar Raphaela'
  - 'Gumede Sbonelo'
institute:  
  - 'University of Cape Town'
  - 'Supervisor: Dr. Birgit Erni'
date: today
format: 
  revealjs:
    incremental: true
    transition: fade
    css: css/styles.css
---

```{r Setup}
options(repos=c(CRAN='https://cloud.r-project.org'))

packs <- c('cmdstanr', 'dplyr', 'forecast', 'fpp2', 'ggfortify', 'gifski', 
			  'ggplot2', 'knitr', 'mvtnorm', 'splines', 'rstan')

for(pack in packs){
	if(!requireNamespace(pack, quietly=TRUE)){
		install.packages(pack, quiet=TRUE)
	}
	library(pack, character.only=TRUE)
}

colors <- c('skyblue', 'seagreen', 'pink', 'cyan', 'gray', 'salmon', 'steelblue')

# Stan setup
rstan_options(auto_write=TRUE)
options(mc.cores=parallel::detectCores())
parallel:::setDefaultClusterOptions(setup_strategy='sequential')
```

## Plot of the dataset

```{r EDA}
df <- EuStockMarkets[, 'FTSE'] # Load data

autoplot(df) +
	labs(title='Daily Closing Prices of FTSE (1991-1998)',
		  x='Time',
		  y='FTSE') + 
	theme_minimal()
```

```{r Partition}
n <- length(df) # Number of observations in the data
train_end_index <- floor(0.7 * n) # (70/30)% split of the data

train <- window(df, end = time(df)[train_end_index]) # Training data
test <- window(df, start = time(df)[train_end_index + 1]) # Testing data
n_train <- length(train) # Number of observations on the training data
n_test <- length(test)  # Number of observations on the testing data
train_time <- as.numeric(time(train)) # Extract the time component for training data
test_time <- as.numeric(time(test)) # Extract the time component for testing data
```

## Linear regression

#### Definition

Linear regression is a model that estimates the relationship between a scalar response (dependent variable) and one or more explanatory variables (independent variable).

#### Form

$$
y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ ... + \beta_mx_{ip} + \varepsilon_i \; (i= 1, 2, ..., n)\\
where\\
\varepsilon \sim \mathcal{N}(0, \sigma^2)
$$

## Linear regression plot

```{r Linear-regression}
linear_regression <- lm(train ~ train_time)
train_preds <- predict(linear_regression, newdata=data.frame(train_time=train_time)) # Spline on training data
test_preds <- predict(linear_regression, newdata=data.frame(train_time=test_time)) # Extrapolate
train_ts <- ts(train_preds, start=start(train), frequency=frequency(train)) # Convert to ts
test_ts <- ts(test_preds, start=start(test), frequency=frequency(test))

autoplot(df) +
	autolayer(train_ts, series='Train Linear Model', size=1.5) +
	autolayer(test_ts, series='Test Linear Model', size=1.5) +
	scale_color_manual(values=c('Train Linear Model'='lightskyblue', 
										 'Test Linear Model'='darkorange')) +
	labs(title='Daily Closing Prices of FTSE (1991-1998)',
		  x='Time',
		  y='FTSE',
		  color='Legend') + 
	theme_minimal() +
	theme(legend.text = element_text(size = 14), 
			legend.title = element_text(size = 16))
```

## Polynomial regression

#### Definition

Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as a polynomial in x.

#### Form

$$
y_i = \beta_0 + \beta_1x_{i} + \beta_2x_{i}^{2} + ... + \beta_mx_{i}^{m} + \varepsilon_i \; (i= 1, 2, ..., n)\\
where\\
\varepsilon \sim \mathcal{N}(0, \sigma^2)
$$

## Polynomial regression plot

```{r Polynomial-regression}
polynomial_regression <- lm(train ~ train_time + I(train_time^2) + I(train_time^3) + I(train_time^4))
train_preds <- predict(polynomial_regression, newdata=data.frame(train_time=train_time)) # Spline on training data
test_preds <- predict(polynomial_regression, newdata=data.frame(train_time=test_time)) # Extrapolate
train_ts <- ts(train_preds, start=start(train), frequency=frequency(train)) # Convert to ts
test_ts <- ts(test_preds, start=start(test), frequency=frequency(test))

autoplot(df) +
	autolayer(train_ts, series='Train Linear Model', size=1.5) +
	autolayer(test_ts, series='Test Linear Model', size=1.5) +
	scale_color_manual(values=c('Train Linear Model'='lightskyblue', 
										 'Test Linear Model'='darkorange')) +
	labs(title='Daily Closing Prices of FTSE (1991-1998)',
		  x='Time',
		  y='FTSE',
		  color='Legend') + 
	theme_minimal() +
	theme(legend.text = element_text(size = 14), 
			legend.title = element_text(size = 16))
```

## B-spline

#### Definition

A B-spline is a piecewise polynomial of order n.

#### Base case

$$
B_{i, 0}(t) := 
\begin{cases} 
    1, & \text{if } t_{i} \leq t < t_{i+1} \\
    0, & \text{otherwise}
\end{cases}
$$

#### Recursive step

$$
B_{i,p}(t) := 
\frac{t - t_{i}}{t_{i+p} - t_{i}} B_{i, p-1}(t) 
+ \frac{t_{i+p+1}-t}{t_{i+p+1}-t_{i+1}} B_{i+1, p-1}(t)\\
where\\
t \; \text{is the covariate and} \; p \; \text{is the degree of the polynomial.}
$$

## B-spline plot

```{r B-spline}
p <- autoplot(df) +
		labs(title='Daily Closing Prices of FTSE (1991-1998)',
			  x='Time',
			  y='FTSE',
			  color='Legend') + 
		theme_minimal() +
		theme(legend.text = element_text(size = 14), 
				legend.title = element_text(size = 16))

color_values <- c()

for(dof in 2:8){
	# Fit a spline model
	spline_model <- lm(train ~ bs(train_time, df=dof))
	
	# Make predictions
	train_preds <- predict(spline_model, newdata=data.frame(train_time=train_time))
	test_preds <- predict(spline_model, newdata=data.frame(train_time=test_time))
	
	# Convert to time series
	train_ts <- ts(train_preds, start=start(train), frequency=frequency(train))
	test_ts <- ts(test_preds, start=start(test), frequency=frequency(test))
	
	# Create series name
	train_name <- paste0('Train Spline df=', dof)
	test_name <- paste0('Test Spline df=', dof)
	
	# Add layers to the plot
	p <- p + 
		autolayer(train_ts, series=train_name, size=1.2) +
		autolayer(test_ts, series=test_name, size=1.2)
	
	# Assign colors to the named vector
	color_idx <- (dof-1) %% length(colors) + 1
	color_values[train_name] <- paste0('light', colors[color_idx])
	color_values[test_name] <- paste0(colors[color_idx])
}

# Add a custom color scale - using the first two colors for each df value
p <- p + scale_color_manual(values=color_values)

# Display the final plot
print(p)
```

## Gaussian process

#### Definition

A time continuous stochastic process $\{X_{t}; \ t \in T\}$ is Gaussian if and only if for every finite set of indices $t_{1},...,t_{k}$ in the index set $T$ $$\mathbf{X}_{t_{1},...,t_{k}} = (X_{t_{1}},...,X_{t_{k}})$$ is a multivariate Gaussian random variable.

#### Meaning

$$f \sim GP(m, k) \rightarrow f_{n} \sim MVN(\mathbf{m}, \mathbf{K})$$ $$\pi(y_{n};f(x_{n}),\phi) \rightarrow \pi(y_{n}; f_{n}, \phi)$$

## Mean vector

$$
\mathbf{m} =
\begin{bmatrix}
\mu(x_{1}^{\text{obs}}) \\
\vdots \\
\mu(x_{N_{\text{obs}}}^{\text{obs}})  \\
\mu(x_{1}^{\text{pred}})\\
\vdots \\
\mu(x_{N_{\text{pred}}}^{\text{pred}})
\end{bmatrix}
$$

## Covariance matrix

$$
\mathbf{K} = 
\begin{bmatrix}
k(x_{1}^{\text{obs}}, x_{1}^{\text{obs}}) & \cdots & k(x_{1}^{\text{obs}}, x_{N_{\text{obs}}}^{\text{obs}}) & \quad & k(x_{1}^{\text{obs}}, x_{1}^{\text{pred}}) & \cdots & k(x_{1}^{\text{obs}}, x_{N_{\text{pred}}}^{\text{pred}}) \\
\vdots & \ddots & \vdots & & \vdots & \ddots & \vdots \\
k(x_{N_{\text{obs}}}^{\text{obs}}, x_{1}^{\text{obs}}) & \cdots & k(x_{N_{\text{obs}}}^{\text{obs}}, x_{N_{\text{obs}}}^{\text{obs}}) & \quad & k(x_{N_{\text{obs}}}^{\text{obs}}, x_{1}^{\text{pred}}) & \cdots & k(x_{N_{\text{obs}}}^{\text{obs}}, x_{N_{\text{pred}}}^{\text{pred}}) \\
k(x_{1}^{\text{pred}}, x_{1}^{\text{obs}}) & \cdots & k(x_{1}^{\text{pred}}, x_{N_{\text{obs}}}^{\text{obs}}) & \quad & k(x_{1}^{\text{pred}}, x_{1}^{\text{pred}}) & \cdots & k(x_{1}^{\text{pred}}, x_{N_{\text{pred}}}^{\text{pred}}) \\
\vdots & \ddots & \vdots & & \vdots & \ddots & \vdots \\
k(x_{N_{\text{pred}}}^{\text{pred}}, x_{1}^{\text{obs}}) & \cdots & k(x_{N_{\text{pred}}}^{\text{pred}}, x_{N_{\text{obs}}}^{\text{obs}}) & \quad & k(x_{N_{\text{pred}}}^{\text{pred}}, x_{1}^{\text{pred}}) & \cdots & k(x_{N_{\text{pred}}}^{\text{pred}}, x_{N_{\text{pred}}}^{\text{pred}})
\end{bmatrix}
$$

## Covariance function

#### Squared exponential

$$
k(x_{i}, x_{j}) = h^{2} exp[-(\frac{x_{i} - x_{j}}{\lambda})^{2}]
$$

## Gaussian process as a Multivariate Normal distribution

:::{#fig-mvn}

![](mvn.png)

Gaussian process as a Multivariate Normal distribution

:::

## Gaussian process plot

:::{#fig-gp}
![](density.png)

Gaussian process plot

:::

```{r Gaussian-process}
#| eval: FALSE

set.seed(1)

x <- train_time
d <- abs(outer(x, x, '-')) # Compute distance matrix d_{ij} = |x_i - x_j|

mx <- x^2 / 4
Sigma_SE <- exp(-d^2 / 2) # Squared exponential kernel

y <- mvtnorm::rmvnorm(1, mean=mx, sigma=Sigma_SE)

plot(
	x, 
	y, 
	ylim=c(-10, 10),
	xlab='Time',
	lwd=2,
	col=1
)

for (i in 1:5) {
	y = mvtnorm::rmvnorm(1, sigma=Sigma_SE)
	lines(x, y, col=i+1, lwd=2)
}
```

```{r Birgit-posterior}
#| eval: FALSE

df <- EuStockMarkets[, 'FTSE'] # Load data
n <- length(df) # Number of observations in the data
train_end_index <- floor(0.7 * n) # (70/30)% split of the data

train <- window(df, end = time(df)[train_end_index]) # Training data
test <- window(df, start = time(df)[train_end_index + 1]) # Testing data
train_time <- as.numeric(time(train)) # Extract the time component for training data
test_time <- as.numeric(time(test)) # Extract the time component for testing data

data_list <- list(
	y_obs=train, 
	x_obs=train_time, 
	N_obs=length(train_time), 
	x2=test_time, 
	N2=length(test_time)
	)

m1 <- cmdstan_model(stan_file='stan/presentaion.stan')

m1.fit <- m1$sample(
	data=data_list, 
	seed=123, 
	chains=4, 
	parallel_chains=2
	)

train_preds <- ?
test_preds <- ?

train_ts <- ts(train_preds, start=start(train), frequency=frequency(train))
test_ts <- ts(test_preds, start=start(test), frequency=frequency(test))

autoplot(df) +
	autolayer(train_ts, series='Train', size=1.5) +
	autolayer(test_ts, series='Test', size=1.5)
```

```{r Prior-model}
#| eval: FALSE

# Gaussian process parameters
alpha_true <- 15
rho_true <- 5
sigma_true <- 4

# Stan parameters
prior_data <- list(
	alpha = alpha_true, 
	rho = rho_true, 
	sigma = sigma_true, 
	N = n_train, 
	x = train_time 
)

# Train prior model
prior_model <- stan(
	file = 'stan_programs/presentation.stan', 
	data = prior_data, 
	warmup = 0, 
	iter = 4000, 
	chains = 1, 
	seed = 494838, 
	algorithm = 'Fixed_param', 
	refresh = 0, 
	verbose = FALSE
)
```

```{r Pior-realizations}
#| eval: FALSE

set.seed(1)
prior_samples <- (extract(prior_model)$f)[1:10, ]
p <- ggplot() + 
	geom_line(aes(x=train_time, y=as.numeric(train)), 
				 color='black')

for(i in 1:10){
	p <- p + 
				geom_line(aes(x=train_time, y=prior_samples[i, ]), 
							 color=scales::hue_pal()(10)[i], linewidth=2)
}

p + 
	labs(title='Prior realizations', x='Time', y='f')  +
	theme_minimal()
```

```{r Posterior-model}
#| eval: FALSE

# Concatenate
x_predict <- c(train_time, test_time) 
N_predict <- length(x_predict) 
observed_idx <- 1:n_train 

# Stan parameters
data <- list(
	N_obs = n_train, 
	y_obs = as.numeric(train), 
	observed_idx = observed_idx, 
	N_predict = N_predict, 
	x_predict = x_predict, 
	alpha = alpha_true, 
	rho = rho_true, 
	sigma = sigma_true 
)

# Train posterior model
normal_fit <- stan(
	file = 'stan_programs/fit_normal.stan', 
	data = data, 
	warmup = 500, 
	iter = 4000, 
	chains = 8, 
	seed = 494838 
)
```
