---
title: 'Gaussian Processes for Time Series Modelling'
author:
  - 'Azar Raphaela'
  - 'Gumede Sbonelo'
institute:  
  - 'University of Cape Town'
  - 'Supervisor: Dr. Birgit Erni'
date: today
format: 
  revealjs:
    incremental: false
    transition: fade
    css: css/styles.css
---

```{r Setup}
options(repos=c(CRAN='https://cloud.r-project.org'))

packs <- c('dplyr', 'forecast', 'fpp2', 'ggfortify', 'gifski', 
			  'ggplot2', 'knitr', 'mvtnorm', 'splines', 'rstan')

for(pack in packs){
	if(!requireNamespace(pack, quietly=TRUE)){
		install.packages(pack, quiet=TRUE)
	}
	library(pack, character.only=TRUE)
}
```

## Plot of the dataset

```{r EDA}
df <- EuStockMarkets[, 'FTSE'] # Load data

autoplot(df) +
	labs(title='Daily Closing Prices of FTSE (1991-1998)',
		  x='Time',
		  y='FTSE') + 
	theme_minimal()
```

```{r Partition}
n <- length(df) # Number of observations in the data
train_end_index <- floor(0.7 * n) # (70/30)% split of the data

train <- window(df, end = time(df)[train_end_index]) # Training data
test <- window(df, start = time(df)[train_end_index + 1]) # Testing data
n_train <- length(train) # Number of observations on the training data
n_test <- length(test)  # Number of observations on the testing data
train_time <- as.numeric(time(train)) # Extract the time component for training data
test_time <- as.numeric(time(test)) # Extract the time component for testing data
```

## Linear regression

#### Definition

Linear regression is a model that estimates the relationship between a scalar response (dependent variable) and one or more explanatory variables (independent variable).

#### Form

$$
y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ ... + \beta_mx_{ip} + \varepsilon_i \; (i= 1, 2, ..., n)\\
where\\
\varepsilon \sim \mathcal{N}(0, \sigma^2)
$$

## Linear regression plot

```{r Linear-regression}
linear_regression <- lm(train ~ train_time)
train_preds <- predict(linear_regression, newdata=data.frame(train_time=train_time)) # Spline on training data
test_preds <- predict(linear_regression, newdata=data.frame(train_time=test_time)) # Extrapolate
train_ts <- ts(train_preds, start=start(train), frequency=frequency(train)) # Convert to ts
test_ts <- ts(test_preds, start=start(test), frequency=frequency(test))

autoplot(df) +
	autolayer(train_ts, series='Train', size=1.5) +
	autolayer(test_ts, series='Test', size=1.5) +
	scale_color_manual(values=c('Train'='lightskyblue', 
										 'Test'='darkorange')) +
	labs(title='Daily Closing Prices of FTSE (1991-1998)',
		  x='Time',
		  y='FTSE',
		  color='Legend') + 
	theme_minimal() +
	theme(legend.text = element_text(size = 14), 
			legend.title = element_text(size = 16))
```

## Polynomial regression

#### Definition

Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as a polynomial in x.

#### Form

$$
y_i = \beta_0 + \beta_1x_{i} + \beta_2x_{i}^{2} + ... + \beta_mx_{i}^{m} + \varepsilon_i \; (i= 1, 2, ..., n)\\
where\\
\varepsilon \sim \mathcal{N}(0, \sigma^2)
$$

## Polynomial regression plot

```{r Polynomial-regression}
polynomial_regression <- lm(train ~ train_time + I(train_time^2) + I(train_time^3))
train_preds <- predict(polynomial_regression, newdata=data.frame(train_time=train_time))
test_preds <- predict(polynomial_regression, newdata=data.frame(train_time=test_time))
train_ts <- ts(train_preds, start=start(train), frequency=frequency(train))
test_ts <- ts(test_preds, start=start(test), frequency=frequency(test))

autoplot(df) +
	autolayer(train_ts, series='Train', size=1.5) +
	autolayer(test_ts, series='Test', size=1.5) +
	scale_color_manual(values=c('Train'='lightskyblue', 'Test'='darkorange')) +
	labs(title='Daily Closing Prices of FTSE (1991-1998)',
		  x='Time',
		  y='FTSE',
		  color='Legend') + 
	theme_minimal() +
	theme(legend.text = element_text(size = 14), 
			legend.title = element_text(size = 16))
```

## B-spline

#### Definition

A B-spline is a piecewise polynomial of order n.

#### Base case

$$
B_{i, 0}(t) := 
\begin{cases} 
    1, & \text{if } t_{i} \leq t < t_{i+1} \\
    0, & \text{otherwise}
\end{cases}
$$

#### Recursive step

$$
B_{i,p}(t) := 
\frac{t - t_{i}}{t_{i+p} - t_{i}} B_{i, p-1}(t) 
+ \frac{t_{i+p+1}-t}{t_{i+p+1}-t_{i+1}} B_{i+1, p-1}(t)\\
where\\
t \; \text{is the covariate and} \; p \; \text{is the degree of the polynomial.}
$$

## B-spline plot

```{r B-spline}
spline_model <- lm(train ~ bs(train_time, df=3))
train_preds <- predict(spline_model, newdata=data.frame(train_time=train_time))
test_preds <- predict(spline_model, newdata=data.frame(train_time=test_time))
train_ts <- ts(train_preds, start=start(train), frequency=frequency(train))
test_ts <- ts(test_preds, start=start(test), frequency=frequency(test))

autoplot(df) +
	autolayer(train_ts, series='Train', size=1.5) +
	autolayer(test_ts, series='Test', size=1.5) +
	scale_color_manual(values=c('Train'='lightskyblue', 'Test'='darkorange')) +
	labs(title='Daily Closing Prices of FTSE (1991-1998)',
		  x='Time',
		  y='FTSE',
		  color='Legend') + 
	theme_minimal() +
	theme(legend.text = element_text(size = 14), 
			legend.title = element_text(size = 16))
```

## Gaussian process

#### Definition

A time continuous stochastic process $\{X_{t}; \ t \in T\}$ is Gaussian if and only if for every finite set of indices $t_{1},...,t_{k}$ in the index set $T$ $$\mathbf{X}_{t_{1},...,t_{k}} = (X_{t_{1}},...,X_{t_{k}})$$ is a multivariate Gaussian random variable.

#### Meaning

$$f \sim GP(m, k) \rightarrow f_{n} \sim MVN(\mathbf{m}, \mathbf{K})$$ $$\pi(y_{n};f(x_{n}),\phi) \rightarrow \pi(y_{n}; f_{n}, \phi)$$

## Example

#### Gaussian process parameters

$$
m(x_{i}) = 0
$$
$$
k(x_{i}, x_{j}) =
\alpha^2exp(-\frac{1}{2}(\frac{|x_{i} - x_{j}|}{\rho})^2)\\
\text{where,}\\
\alpha \; 
\text{controls the marginal variability} \\
\text{and}\\
\rho \;
\text{controls how quickly the correlations between function values decay.}
$$

#### Multivariate normal parameters

$$
m_{i} = m(x_i) = 0 \quad (i=1, .., k)
$$
$$
K_{i,j} = k(x_{i}, x_{j}) \quad (i,j = 1, .., k)
$$

## Gaussian process as a multivariate normal distribution

:::{.no-caption}
![](mvn.png){width=1500px height=500px}
:::

## Predictions

#### Mean vector

<div style="overflow-x: auto; font-size: 80%;">

$$
\mathbf{m} =
\begin{bmatrix}
\mu(x_{1}^{\text{obs}}) \\
\vdots \\
\mu(x_{N_{\text{obs}}}^{\text{obs}})  \\
\mu(x_{1}^{\text{pred}})\\
\vdots \\
\mu(x_{N_{\text{pred}}}^{\text{pred}})
\end{bmatrix}
$$
</div>

## Predictions

#### Covariance matrix

<div style="overflow-x: auto; font-size: 80%;">

$$
\mathbf{K} = 
\begin{bmatrix}
k(x_{1}^{\text{obs}}, x_{1}^{\text{obs}}) & \cdots & k(x_{1}^{\text{obs}}, x_{N_{\text{obs}}}^{\text{obs}}) & \quad & k(x_{1}^{\text{obs}}, x_{1}^{\text{pred}}) & \cdots & k(x_{1}^{\text{obs}}, x_{N_{\text{pred}}}^{\text{pred}}) \\
\vdots & \ddots & \vdots & & \vdots & \ddots & \vdots \\
k(x_{N_{\text{obs}}}^{\text{obs}}, x_{1}^{\text{obs}}) & \cdots & k(x_{N_{\text{obs}}}^{\text{obs}}, x_{N_{\text{obs}}}^{\text{obs}}) & \quad & k(x_{N_{\text{obs}}}^{\text{obs}}, x_{1}^{\text{pred}}) & \cdots & k(x_{N_{\text{obs}}}^{\text{obs}}, x_{N_{\text{pred}}}^{\text{pred}}) \\
k(x_{1}^{\text{pred}}, x_{1}^{\text{obs}}) & \cdots & k(x_{1}^{\text{pred}}, x_{N_{\text{obs}}}^{\text{obs}}) & \quad & k(x_{1}^{\text{pred}}, x_{1}^{\text{pred}}) & \cdots & k(x_{1}^{\text{pred}}, x_{N_{\text{pred}}}^{\text{pred}}) \\
\vdots & \ddots & \vdots & & \vdots & \ddots & \vdots \\
k(x_{N_{\text{pred}}}^{\text{pred}}, x_{1}^{\text{obs}}) & \cdots & k(x_{N_{\text{pred}}}^{\text{pred}}, x_{N_{\text{obs}}}^{\text{obs}}) & \quad & k(x_{N_{\text{pred}}}^{\text{pred}}, x_{1}^{\text{pred}}) & \cdots & k(x_{N_{\text{pred}}}^{\text{pred}}, x_{N_{\text{pred}}}^{\text{pred}})
\end{bmatrix}
$$

</div>

## Compactly

<div style="display: flex; justify-content: space-around; align-items: center;">

<div style="flex: 1; text-align: center;">
$$
\mathbf{m} = 
\begin{bmatrix} 
\mathbf{m}_{\text{obs}} \\ 
\mathbf{m}_{\text{pred}} 
\end{bmatrix}
$$
</div>

<div style="flex: 1; text-align: center;">
$$
\mathbf{K} = 
\begin{bmatrix}
\mathbf{K}_{\text{obs}} \quad \mathbf{K}_{\text{mix}} \\
(\mathbf{K}_{\text{mix}})^{\top} \quad \mathbf{K}_{\text{pred}}
\end{bmatrix}
$$
</div>

</div>

#### Conditional density

$$ 
\pi(\mathbf{f}_{pred} | \mathbf{f}_{obs}) = 
\text{multi-normal}(\boldsymbol{\mu}, \boldsymbol{\Sigma})
$$

#### with the location parameters

$$
\boldsymbol{\mu} = 
\mathbf{m}_{pred} + 
(\mathbf{K}_{mix})^{T} \cdot (\mathbf{K}_{obs})^{-1} \cdot ( \mathbf{f}_{obs} - \mathbf{m}_{obs})
$$

#### and the covariance matrix parameters

$$
\boldsymbol{\Sigma} = 
\mathbf{K}_{pred} - 
(\mathbf{K}_{mix})^{T} \cdot (\mathbf{K}_{obs})^{-1} \cdot \mathbf{K}_{mix}
$$

## Gaussian process plot

:::{.no-caption}
![](density.png){width=1500px height=500px}
:::


## Gaussian process posterior plot

:::{.no-caption}
![](quantiles.png){width=1500px height=500px}
:::

## Looking ahead 

The multidimensional properties of Gaussian Process Regression can be extended to predict a group of commodity prices. 

While traditional models like ARIMA estimate commodities independently, 
GPR allows us to capture correlations between prices across different commodities.


## References

- Rasmussen & Williams. Gaussian Processes for Machine Learning. MIT. 
https://direct.mit.edu/books/book/2320/Gaussian-Processes-for-Machine-Learning
- Rasmussen 2006. Gaussian Processes in Machine Learning.
- Robertsetal. Gaussian processes for time-series modelling. Philosophical Transactions of the Royal Society.
- https://betanalpha.github.io/assets/case_studies/gaussian_processes.html
- Ebden 2008. Gaussian Processes for Regression: A Quick Introduction.
- Neal, R.M. 1997. MonteCarlo Implementation of Gaussian Process Models for Bayesian Regression and Classification. arXiv.
https://doi.org/10.48550/arXiv.physics/9701026
- Neal, R.M. 1998. Regression and Classification Using Gaussian Process Priors in J.M.Bernardoetal. (eds) Bayesian Statistics 6. Oxford University Press.
